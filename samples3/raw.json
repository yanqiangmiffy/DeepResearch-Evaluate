{
    "title": "RAG评估研究报告",
    "create_time": 1746642087.483099,
    "update_time": 1746670603.62919,
    "mapping": {
        "client-created-root": {
            "id": "client-created-root",
            "message": null,
            "parent": null,
            "children": [
                "5e10abc4-e69c-4b33-abd7-5868eddb581a"
            ]
        },
        "5e10abc4-e69c-4b33-abd7-5868eddb581a": {
            "id": "5e10abc4-e69c-4b33-abd7-5868eddb581a",
            "message": {
                "id": "5e10abc4-e69c-4b33-abd7-5868eddb581a",
                "author": {
                    "role": "system",
                    "name": null,
                    "metadata": {}
                },
                "create_time": null,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        ""
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 0.0,
                "metadata": {
                    "is_visually_hidden_from_conversation": true
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "client-created-root",
            "children": [
                "ebdfbc88-6e6b-4018-8e14-6c5dcb6e33be"
            ]
        },
        "ebdfbc88-6e6b-4018-8e14-6c5dcb6e33be": {
            "id": "ebdfbc88-6e6b-4018-8e14-6c5dcb6e33be",
            "message": {
                "id": "ebdfbc88-6e6b-4018-8e14-6c5dcb6e33be",
                "author": {
                    "role": "user",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746642086.774,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "请生成一份关于检索增强生成（RAG）系统评估的深度研究报告，需涵盖以下内容：\n\nRAG在自然语言处理中的关键作用及其混合结构的独特挑战；\n\n现有RAG评估方法的系统性分析框架（如RGAR框架），包括检索（Retrieval）、生成（Generation）和附加要求（Additional Requirement）的量化指标（如相关性、准确性、真实性）；\n\n当前RAG基准的局限性，以及如何通过输出与真实情况对的对比解决评估问题；\n\n未来研究的潜在方向，以改进RAG评估的标准化与全面性。\n要求：结合最新研究进展，提供具体指标、案例分析和领域内权威论文的引用支持。"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": null,
                "weight": 1.0,
                "metadata": {
                    "selected_github_repos": [],
                    "system_hints": [
                        "research"
                    ],
                    "serialization_metadata": {
                        "custom_symbol_offsets": []
                    },
                    "request_id": "93c2bcb3ac177368-DFW",
                    "message_source": null,
                    "timestamp_": "absolute",
                    "message_type": null
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "5e10abc4-e69c-4b33-abd7-5868eddb581a",
            "children": [
                "f0a71e05-f52d-4fc3-8ce1-03892bcbd0a7"
            ]
        },
        "f0a71e05-f52d-4fc3-8ce1-03892bcbd0a7": {
            "id": "f0a71e05-f52d-4fc3-8ce1-03892bcbd0a7",
            "message": {
                "id": "f0a71e05-f52d-4fc3-8ce1-03892bcbd0a7",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746642099.219392,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "为了为您撰写一份深入的关于检索增强生成（RAG）系统评估的研究报告，我想确认以下几个细节：\n\n1. 您是否希望报告聚焦于特定应用场景中的RAG系统（如问答、摘要生成、对话系统等），还是以通用RAG结构为主？\n2. 您是否有首选的格式（例如学术论文格式、技术白皮书或PowerPoint汇报型结构）？\n3. 是否希望报告包括图表（如评估框架结构图、指标对比图等）或可视化示意？\n4. 是否对引用来源有偏好（如ACL、EMNLP、NeurIPS等会议论文）？\n\n确认这些细节后，我将开始研究任务。"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 1.0,
                "metadata": {
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "ebdfbc88-6e6b-4018-8e14-6c5dcb6e33be",
                    "request_id": "93c2bcb3ac177368-DFW",
                    "timestamp_": "absolute",
                    "model_switcher_deny": []
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "ebdfbc88-6e6b-4018-8e14-6c5dcb6e33be",
            "children": [
                "33b24686-0dd1-41e6-983f-e7d7a4afb257"
            ]
        },
        "33b24686-0dd1-41e6-983f-e7d7a4afb257": {
            "id": "33b24686-0dd1-41e6-983f-e7d7a4afb257",
            "message": {
                "id": "33b24686-0dd1-41e6-983f-e7d7a4afb257",
                "author": {
                    "role": "tool",
                    "name": "research_kickoff_tool.clarify_with_text",
                    "metadata": {}
                },
                "create_time": 1746642099.219551,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        ""
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 1.0,
                "metadata": {
                    "is_visually_hidden_from_conversation": true,
                    "command": "clarify_with_text",
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "f0a71e05-f52d-4fc3-8ce1-03892bcbd0a7",
                    "request_id": "93c2bcb3ac177368-DFW",
                    "timestamp_": "absolute"
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "f0a71e05-f52d-4fc3-8ce1-03892bcbd0a7",
            "children": [
                "4991591e-faea-40d7-a2eb-647419b1ccb7"
            ]
        },
        "4991591e-faea-40d7-a2eb-647419b1ccb7": {
            "id": "4991591e-faea-40d7-a2eb-647419b1ccb7",
            "message": {
                "id": "4991591e-faea-40d7-a2eb-647419b1ccb7",
                "author": {
                    "role": "user",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746642135.311,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "以通用RAG结构为主\n学术论文格式  \n报告包括图表\n没有"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": null,
                "weight": 1.0,
                "metadata": {
                    "selected_github_repos": [],
                    "system_hints": [
                        "research"
                    ],
                    "serialization_metadata": {
                        "custom_symbol_offsets": []
                    },
                    "request_id": "93c2bde2fbfa7368-DFW",
                    "message_source": null,
                    "timestamp_": "absolute",
                    "message_type": null
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "33b24686-0dd1-41e6-983f-e7d7a4afb257",
            "children": [
                "1077b9b5-997a-4ab1-9784-b3c0676f5b12"
            ]
        },
        "1077b9b5-997a-4ab1-9784-b3c0676f5b12": {
            "id": "1077b9b5-997a-4ab1-9784-b3c0676f5b12",
            "message": {
                "id": "1077b9b5-997a-4ab1-9784-b3c0676f5b12",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746642147.909125,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "明白了。我将撰写一份关于通用RAG结构评估的学术格式研究报告，涵盖其在自然语言处理中的关键作用、混合结构所带来的挑战、系统性评估框架（如RGAR）、当前基准的局限性、未来研究方向，并结合最新研究进展、具体评估指标、图表展示和权威论文引用。\n我会在完成后通知您以供审阅。"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": null,
                "weight": 1.0,
                "metadata": {
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "4991591e-faea-40d7-a2eb-647419b1ccb7",
                    "request_id": "93c2bde2fbfa7368-DFW",
                    "timestamp_": "absolute",
                    "model_switcher_deny": []
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "4991591e-faea-40d7-a2eb-647419b1ccb7",
            "children": [
                "263e3ba7-4adf-44f5-84d0-9424d97ada24"
            ]
        },
        "263e3ba7-4adf-44f5-84d0-9424d97ada24": {
            "id": "263e3ba7-4adf-44f5-84d0-9424d97ada24",
            "message": {
                "id": "263e3ba7-4adf-44f5-84d0-9424d97ada24",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746642135.929855,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "{\n  \"task_violates_safety_guidelines\": false,\n  \"user_def_doesnt_want_research\": false,\n  \"response\": \"明白了。我将撰写一份关于通用RAG结构评估的学术格式研究报告，涵盖其在自然语言处理中的关键作用、混合结构所带来的挑战、系统性评估框架（如RGAR）、当前基准的局限性、未来研究方向，并结合最新研究进展、具体评估指标、图表展示和权威论文引用。\\n我会在完成后通知您以供审阅。\",\n  \"title\": \"检索增强生成（RAG）系统评估的研究报告\",\n  \"prompt\": \"撰写一篇学术论文格式的深度研究报告，主题为通用结构下的检索增强生成（RAG）系统的评估方法。内容包括：\\n\\n1. RAG系统在自然语言处理（NLP）中的关键作用，尤其是在开放领域问答、对话系统、自动摘要等任务中的表现；\\n2. RAG的混合结构（检索+生成）所带来的独特挑战，如检索器与生成器之间的信息接口问题、不一致性、知识对齐等；\\n3. 现有RAG评估方法的系统性分析框架，如RGAR框架，细致介绍评估维度：\\n   - Retrieval 方面的相关性（relevance）、多样性（diversity）、覆盖度（coverage）等指标；\\n   - Generation 方面的准确性（accuracy）、流畅性（fluency）、事实一致性（factual consistency）、信息保留度等指标；\\n   - Additional Requirement 方面包括领域适应性（domain robustness）、延迟、用户偏好匹配等；\\n   - 同时需引入示意图展示评估流程结构；\\n4. 当前RAG评估基准存在的局限性，如难以针对生成输出与真实事实的对应关系进行有效量化，现有人工评估与自动指标脱节等问题；\\n   - 说明如何通过引入事实对比（fact-grounded comparison）或知识图谱支持的验证机制来提升评估质量；\\n5. 未来研究方向，包括标准化评估数据集、多层次评价协议、领域通用性评估、多模态RAG系统的评估扩展等。\\n\\n报告应引用近年权威会议（如ACL、EMNLP、NAACL、NeurIPS等）的研究成果，并配合适当图表说明各项方法和指标的结构及对比结果。\\n报告语言为中文，整体结构包括摘要、引言、方法综述、分析与讨论、未来展望与参考文献。\"\n}"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": false,
                "weight": 1.0,
                "metadata": {
                    "finish_details": {
                        "type": "stop",
                        "stop_tokens": [
                            200007
                        ]
                    },
                    "is_complete": true,
                    "citations": [],
                    "content_references": [],
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "1077b9b5-997a-4ab1-9784-b3c0676f5b12",
                    "request_id": "93c2bde2fbfa7368-DFW",
                    "timestamp_": "absolute",
                    "model_switcher_deny": [
                        {
                            "slug": "o1-mini",
                            "context": "regenerate",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        },
                        {
                            "slug": "o1-mini",
                            "context": "conversation",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        }
                    ],
                    "is_loading_message": true
                },
                "recipient": "research_kickoff_tool.start_research_task",
                "channel": null
            },
            "parent": "1077b9b5-997a-4ab1-9784-b3c0676f5b12",
            "children": [
                "1d9fe8d3-bbfd-4709-8240-fc031612f651"
            ]
        },
        "1d9fe8d3-bbfd-4709-8240-fc031612f651": {
            "id": "1d9fe8d3-bbfd-4709-8240-fc031612f651",
            "message": {
                "id": "1d9fe8d3-bbfd-4709-8240-fc031612f651",
                "author": {
                    "role": "tool",
                    "name": "research_kickoff_tool.start_research_task",
                    "metadata": {}
                },
                "create_time": 1746642155.379299,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": []
                },
                "status": "finished_successfully",
                "end_turn": false,
                "weight": 1.0,
                "metadata": {
                    "async_task_title": "检索增强生成（RAG）系统评估的研究报告",
                    "async_task_prompt": "撰写一篇学术论文格式的深度研究报告，主题为通用结构下的检索增强生成（RAG）系统的评估方法。内容包括：\n\n1. RAG系统在自然语言处理（NLP）中的关键作用，尤其是在开放领域问答、对话系统、自动摘要等任务中的表现；\n2. RAG的混合结构（检索+生成）所带来的独特挑战，如检索器与生成器之间的信息接口问题、不一致性、知识对齐等；\n3. 现有RAG评估方法的系统性分析框架，如RGAR框架，细致介绍评估维度：\n   - Retrieval 方面的相关性（relevance）、多样性（diversity）、覆盖度（coverage）等指标；\n   - Generation 方面的准确性（accuracy）、流畅性（fluency）、事实一致性（factual consistency）、信息保留度等指标；\n   - Additional Requirement 方面包括领域适应性（domain robustness）、延迟、用户偏好匹配等；\n   - 同时需引入示意图展示评估流程结构；\n4. 当前RAG评估基准存在的局限性，如难以针对生成输出与真实事实的对应关系进行有效量化，现有人工评估与自动指标脱节等问题；\n   - 说明如何通过引入事实对比（fact-grounded comparison）或知识图谱支持的验证机制来提升评估质量；\n5. 未来研究方向，包括标准化评估数据集、多层次评价协议、领域通用性评估、多模态RAG系统的评估扩展等。\n\n报告应引用近年权威会议（如ACL、EMNLP、NAACL、NeurIPS等）的研究成果，并配合适当图表说明各项方法和指标的结构及对比结果。\n报告语言为中文，整体结构包括摘要、引言、方法综述、分析与讨论、未来展望与参考文献。",
                    "async_task_type": "research",
                    "async_task_status_messages": {
                        "initial": "检索增强生成（RAG）系统评估的研究报告",
                        "completed_with_time": "Research completed in 5m",
                        "completed_no_time": "已完成研究",
                        "completed": "已研究的时间 ",
                        "error": "无法研究",
                        "cancelled": "研究已取消"
                    },
                    "b1de6e2_s": true,
                    "command": "start_research_task",
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "263e3ba7-4adf-44f5-84d0-9424d97ada24",
                    "request_id": "93c2bde2fbfa7368-DFW",
                    "timestamp_": "absolute",
                    "async_task_id": "deepresch_681ba4eb92bc8191949c31b27d216ba4",
                    "async_task_conversation_id": "681ba4eb-9c0c-8004-acd3-58e21c458e89",
                    "async_task_created_at": "2025-05-07 18:22:36.363067+00:00",
                    "deep_research_version": "mini"
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "263e3ba7-4adf-44f5-84d0-9424d97ada24",
            "children": [
                "9895a557-a971-404b-99c0-5d9986fdc70f"
            ]
        },
        "9895a557-a971-404b-99c0-5d9986fdc70f": {
            "id": "9895a557-a971-404b-99c0-5d9986fdc70f",
            "message": {
                "id": "9895a557-a971-404b-99c0-5d9986fdc70f",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746642443.95323,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "# 摘要  \n检索增强生成（Retrieval-Augmented Generation，RAG）通过将大规模语言模型（LLM）与外部知识库检索相结合，有效提升了生成结果的准确性和信息丰富度【69†L58-L64】【24†L39-L45】。RAG方法广泛应用于开放领域问答、知识驱动对话、自动摘要等知识密集型任务【22†L60-L68】【24†L39-L45】。然而，其混合“检索+生成”结构也给评估带来了独特挑战：需要同时衡量检索结果的质量与生成内容的真实性和一致性，同时兼顾系统的效率等附加需求【69†L58-L64】【25†L1-L9】。本文综述了RAG系统的评估方法，采用RGAR（Retrieval、Generation、Additional Requirement）分析框架，详细介绍了各评估维度：检索部分考察相关性、多样性、覆盖度等指标；生成部分关注答案的准确性、流畅性、事实一致性与信息保留度等指标；附加需求则包括领域鲁棒性、响应延迟、用户偏好一致性等。针对现有评估基准的不足，如自动指标难以量化生成内容与真实事实的一致性以及与人工评估脱节等问题，我们讨论了引入事实对比和知识图谱验证等方法来改进评估。最后展望了未来研究方向，包括标准化评估数据集构建、多层次评估协议设计、领域通用性与多模态RAG系统评估等。\n\n## 引言  \n检索增强生成（RAG）是一种将LLM与外部检索模块结合的技术框架，通过检索相关知识片段为生成模型提供背景信息，从而显著提高回答的准确性和内容丰富度【69†L58-L64】【24†L39-L45】。Lewis等人在 NeurIPS2020 上提出的 RAG 模型，在多个开放领域问答任务上达到了最新水平，证明了该方法在提升问答准确率和减少模型“幻觉”（hallucination）方面的有效性【24†L39-L45】。同时，Choi等在2024年EMNLP工业界研讨会上指出，检索增强技术也已成功应用于开放领域对话问答，通过引入检索结果可以提升对话系统的上下文连贯性和答复质量【46†L66-L70】。RAG系统通常由**检索组件**和**生成组件**两部分构成。检索组件负责从大量文档或知识库中找到与输入查询最相关的信息，生成组件则利用这些检索到的文档通过LLM进行综合推理和回答生成【45†L67-L75】【71†L172-L180】。其典型流程包括：脱机的语料索引与预处理、在线阶段的查询理解（如意图识别、查询扩写）、检索相关文档、文档重排序、最后将检索结果与查询一起输入LLM进行响应生成等环节【45†L67-L75】【71†L177-L180】。在这个过程中，LLM不再仅凭自身参数生成内容，而是以检索到的知识为依据，显著降低了独立生成带来的误差和幻觉风险【71†L177-L180】【69†L58-L64】。 \n\n由于RAG系统同时包含检索与生成两种不同性质的模块，其评估比单一模块更为复杂【69†L58-L64】【25†L1-L9】。在检索端，需要衡量系统检索到的文档是否与查询真实需求相符；在生成端，需要判断基于检索信息生成的答案是否准确、流畅且与事实一致。此外，检索和生成之间的接口问题也可能导致信息丢失或不一致，如检索结果和生成回答之间的知识对齐（knowledge alignment）问题等【69†L58-L64】【25†L1-L9】。正因为如此，RAG系统的评估既要独立评估各组件性能，又要关注整体系统的协同效果和实际部署效率（如响应延迟、用户体验等）【45†L92-L100】【25†L1-L9】。本报告将以最新文献为依据，系统分析RAG系统的评估框架和指标，探讨现有方法的优势与不足，并展望未来发展方向。\n\n## 评估方法综述  \n针对RAG评估的复杂性，已有工作提出了系统性的分析框架。比如，文献中引入了**RGAR**（Retrieval, Generation, Additional Requirement）框架来分类整理评估方法【69†L41-L47】【4†L285-L293】。在该框架下，评估涵盖三个层面：**检索**部分关注系统检索的正确性和广度；**生成**部分关注答案文本的质量与一致性；**附加需求**则包括系统部署时的其他指标如延迟、鲁棒性和用户满意度等【4†L285-L293】【30†L363-L366】。为便于说明，这里首先简要复述RAG系统流程：  \n\n【37†embed_image】图示一个典型的RAG系统工作流程。在线阶段，系统首先进行查询理解（可能包括用户意图识别和查询扩写），然后检索器在构建好的索引（如Wikipedia文本、知识库片段等）中搜索相关文档【45†L67-L75】【71†L172-L180】。检索模块可以采用稀疏检索、密集检索、图检索等多种策略【71†L172-L180】，并可对结果进行重排序或合并不同来源的信息【71†L172-L180】。接着，选取的相关文档与用户查询一起被送入生成模块，由LLM进行推理与回答生成【71†L177-L180】【45†L80-L88】。这种设计使得LLM能够“查询而非遗忘”，在生成回答时综合外部知识，从而减少盲目记忆输出的错误【71†L177-L180】【69†L58-L64】。\n\n### 检索评估指标  \n在RGAR框架下，检索评估主要衡量检索结果与查询或参考答案之间的一致性。常见指标包括**相关性（Relevance）**、**覆盖度（Coverage）/多样性（Diversity）**等：  \n- **相关性**：指检索得到的文档集合与原始查询需求之间的匹配程度，通常使用命中率（Accuracy/Hit@K）、精确率（Precision@K）、召回率（Recall@K）、MRR、nDCG等指标来度量检索结果的准确性和优先级【41†L249-L257】。例如，Relevance可量化“Relevant Docs ⇔ Query”的匹配度，反映系统检索到的信息与查询需求的贴合度【41†L249-L257】。  \n- **多样性和覆盖度**：指检索结果集合是否从多个角度全面涵盖了潜在的相关信息【41†L249-L257】【32†L403-L411】。文献中也称之为Comprehensiveness，它评估系统检索到的文档群是否包含尽可能多的相关事实，避免遗漏关键知识【41†L249-L257】【32†L403-L411】。通常可以通过度量检索结果与所有相关文档的重叠比例来定义覆盖度【32†L403-L411】。此外，还可分析检索结果之间的多样性，即结果间的文本异质性，以确保提供多元的知识来源。  \n- **正确性**：对于带有候选答案集的任务，也可考察检索排序中正确文档排在前列的能力，类似于一般检索任务中的排序准确性。  \n\n### 生成评估指标  \n生成阶段评估更侧重语言质量和事实正确性，关键指标包括：**准确性（Accuracy）**、**流畅性（Fluency）**、**事实一致性（Factual Consistency/Faithfulness）**和**信息保留度**等【30†L334-L344】【30†L348-L352】【32†L368-L374】。具体地：  \n- **回答相关性与准确性**：评价生成的回答与输入查询的匹配度，以及回答内容本身是否正确。RGAR框架中称之为Response→Query的“相关性”以及与参考答案的“正确性”指标【30†L334-L344】【30†L348-L352】。其中“正确性”通常通过与人工标注的标准答案做对比（如准确率、F1等QA指标）来衡量回答的正确性和上下文适当性【30†L348-L352】。  \n- **事实一致性（可信度）**：指生成回答所包含信息与检索到的事实或外部知识库之间的一致程度【30†L339-L344】【28†L37-L40】。文献中也称“Faithfulness”，用于测量回答中陈述是否忠实反映检索文档提供的内容【30†L339-L344】【28†L37-L40】。高事实一致性意味着回答中的陈述在检索结果中可以找到支撑，降低了错误信息或“幻觉”的风险。评估方法通常包括基于知识库的事实查证或对答案中的实体和关系进行验证。  \n- **流畅性**：衡量回答文本的可读性、连贯性和语言质量。常用BLEU、ROUGE、METEOR等自动指标来评估生成文本与参考答案在词汇层面的相似度，但BLEU等指标并不直接考虑句子结构的语法通顺性【32†L368-L374】，因此在评估生成质量时往往需要结合人工评分或更语义化的指标（如BERTScore）来补充。  \n- **信息保留度**：指生成回答在多大程度上保留了检索结果或原始文档中的重要信息。这一指标侧重评估回答的全面性与详实度，确保检索到的关键知识没有在生成过程中被遗漏。例如，可以用问答准确率或摘要覆盖率等方式间接衡量回答是否包含了核心信息。  \n\n### 附加需求指标  \n除了以上核心指标，RAG系统还需满足实际应用的其他要求，常见附加需求包括：**领域鲁棒性（Domain Robustness）**、**系统延迟（Latency）**、**用户偏好匹配**等【30†L354-L360】【30†L363-L366】。例如，领域鲁棒性关注模型在不同知识域或任务上的泛化能力；延迟指标评估完成一次检索与生成所需的时间，对实时响应系统尤为关键【30†L363-L366】；“用户偏好”评估则考察生成回答在可读性、完整度、多样性等方面是否符合人类偏好【30†L354-L360】。RGAR框架下，还提出了诸如**否定回答能力**、**抗干扰能力**、**反事实健壮性**等指标，用于测试模型在无答案场景或面对错误检索内容时的行为【30†L365-L374】。总之，一个完整的RAG评估体系需要横跨检索与生成，并考虑实际部署的多种性能指标，从而对系统进行多维度综合评估。  \n\n## 分析与讨论  \n尽管已有多种评估指标和基准被提出，当前RAG评估仍面临若干局限。首先，常用的自动评估指标（如BLEU、ROUGE、精确率、F1等）主要衡量生成回答与参考答案在表面形式上的吻合程度，难以直接反映回答是否与真实世界事实一致【32†L368-L374】【28†L37-L40】。比如BLEU指标并不考虑语法流畅性【32†L368-L374】，而ROUGE主要度量词汇重叠，对事实正确性缺乏专门判断。此外，RAG系统输出的参考标准往往难以穷尽，单一标准答案可能无法覆盖所有正确答案，这进一步限制了自动指标的适用性。其次，人工评估成本高昂且主观性强，使得大量研究依赖自动指标和有限的人类标注两者割裂的情况时有发生，评估结果与真实用户体验可能存在偏差。  \n\n为了提高评估质量，研究者开始探索引入**事实驱动对比**和**知识图谱验证**等机制。一种思路是**对生成答案进行事实检索比对**：将回答中的论断与检索到的文档或独立知识库中的信息进行对齐，检查每条陈述是否有证据支持，从而量化回答的可信度【28†L37-L40】。例如，可以利用大型知识图谱或数据库验证回答中的实体和关系是否正确存在，将错误或不一致的回答进行惩罚。另一种思路是采用**评价模型**（如LLM或专门训练的判别器）对RAG输出进行审查，即“让LLM评判LLM”的方式。如同[56]所提出的**Groundedness**评估，训练模型判断回答与检索上下文的一致性，或使用人工标注数据微调判别器来衡量回答的真实性。这些方法在一定程度上可以弥补传统指标的不足，使评估更贴近人类对事实准确性的要求。此外，构建更丰富的评估数据集也至关重要，比如设计包含潜在干扰信息或跨领域查询的评测集，以测试RAG系统在真实复杂场景下的性能。综上，事实比对、知识图谱检验和智能评价器等手段有助于提升RAG评估的有效性和可靠性，但仍需更多研究来完善这些技术。  \n\n## 未来展望  \n未来RAG评估研究有多条重要发展方向值得关注。首先，需要构建**标准化且多样化的评估数据集**，覆盖更多任务类型和领域。现有公开基准多为静态的开放域问答数据集（如Natural Questions、HotpotQA等），其领域局限性和时效性不足【61†L55-L62】。未来可开发跨领域、多任务的混合数据集，并兼顾语言多样性，提高评估框架的通用性【60†L1089-L1093】【61†L55-L62】。其次，应设计**多层次评估协议**，对RAG系统进行精细化分解评测：不仅评估整体回答质量，还应独立评估检索质量、生成质量及其交互效应，形成分层次的评测流程【60†L1079-L1088】。此外，**领域通用性评估**是重要方向，即验证RAG系统在多个知识域间的稳定性和迁移能力。为此，可进行跨领域的基准测试和领域自适应评估。最后，随着多模态RAG的发展，评估体系也需扩展到**多模态场景**。已有工作如 RAG-Check 提出了针对图文检索-生成流程的评估方法，通过**相关度评分（Relevancy Score）**和**正确性评分（Correctness Score）**分别衡量多模态检索结果与回答的表现【63†L61-L70】。未来研究应进一步开发综合度量手段来评估图像、文本等多模态输入下的RAG系统性能。综上所述，通过构建更全面的数据集、完善评估协议并纳入最新的评估技术，可推动RAG系统评估向更加规范、可靠和通用的方向发展。\n\n## 参考文献  \n[1] Lewis P 等. 检索增强生成用于知识密集型NLP任务[C]//NeurIPS 2020.  \n[2] Choi B 等. 开放领域对话问答的检索增强对话数据集[C]//EMNLP 2024 (Industry Track).  \n[3] Gan A 等. RAG评估在大语言模型时代：全面综述[J]. arXiv预印本, 2025, arXiv:2504.14891.  \n[4] Mortaheb M 等. RAG-Check: 多模态检索增强生成性能评估[J]. arXiv预印本, 2025, arXiv:2501.03995."
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 1.0,
                "metadata": {
                    "citations": [
                        {
                            "start_ix": 95,
                            "end_ix": 107,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 107,
                            "end_ix": 119,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                                "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                                "text": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 24,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 39,
                                    "end_line_num": 45
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 156,
                            "end_ix": 168,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "RAG | Papers With Code",
                                "url": "https://paperswithcode.com/task/rag#:~:text=coherence%20of%20generated%20text%2C%20especially,MS%20MARCO%2C%20TriviaQA%2C%20and%20SQuAD",
                                "text": "coherence of generated text, especially in tasks requiring detailed knowledge or long context handling. RAG is particularly useful in open-domain question answering, knowledge-grounded dialogue, and summarization tasks. The retrieval step helps the model to access and incorporate external information, making it less reliant on memorized knowledge and better suited for generating responses based on the latest or domain-specific information. The performance of RAG systems is usually measured using metrics such as precision, recall, F1 score, BLEU score, and exact match. Some popular datasets for evaluating RAG models include Natural Questions, MS MARCO, TriviaQA, and SQuAD.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 22,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 60,
                                    "end_line_num": 68
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 168,
                            "end_ix": 180,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                                "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                                "text": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 24,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 39,
                                    "end_line_num": 45
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 249,
                            "end_ix": 261,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 261,
                            "end_ix": 271,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                                "text": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 25,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1,
                                    "end_line_num": 9
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 645,
                            "end_ix": 657,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 657,
                            "end_ix": 669,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                                "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                                "text": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 24,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 39,
                                    "end_line_num": 45
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 767,
                            "end_ix": 779,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                                "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                                "text": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 24,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 39,
                                    "end_line_num": 45
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 859,
                            "end_ix": 871,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "",
                                "url": "https://aclanthology.org/2024.emnlp-industry.108.pdf#:~:text=open,for%20answering%20questions%20in%20conversational",
                                "text": "open-domain QA tasks. These models, referred to as retrieval-augmented generation (RAG) mod\u0002els, offer superior performance and flexibility (Raf\u0002fel et al., 2020; Min et al., 2020; Lewis et al., 2020b). Moreover, generative models are well\u0002suited for answering questions in conversational",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 46,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 66,
                                    "end_line_num": 70
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 964,
                            "end_ix": 976,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=There%20are%20many%20types%20of,to%20the%20user%E2%80%99s%20query%2C%20often",
                                "text": "There are many types of RAGs with different structures [RAGServey]. Figure 1 illustrates the structure of the RAG systems we used in this paper, which concentrated on two functional components and four phases. The retrieval component is tasked with extracting relevant information from a vast array of external knowledge sources. This process involves two main phases: indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [RAGServey, FAISS, ColBERT]. The search phase then utilizes these indexes to fetch documents most relevant to the user’s query, often",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 67,
                                    "end_line_num": 75
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 976,
                            "end_ix": 990,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                                "text": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 172,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1072,
                            "end_ix": 1084,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=There%20are%20many%20types%20of,to%20the%20user%E2%80%99s%20query%2C%20often",
                                "text": "There are many types of RAGs with different structures [RAGServey]. Figure 1 illustrates the structure of the RAG systems we used in this paper, which concentrated on two functional components and four phases. The retrieval component is tasked with extracting relevant information from a vast array of external knowledge sources. This process involves two main phases: indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [RAGServey, FAISS, ColBERT]. The search phase then utilizes these indexes to fetch documents most relevant to the user’s query, often",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 67,
                                    "end_line_num": 75
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1084,
                            "end_ix": 1098,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                                "text": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 177,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1154,
                            "end_ix": 1168,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                                "text": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 177,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1168,
                            "end_ix": 1180,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1222,
                            "end_ix": 1234,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1234,
                            "end_ix": 1244,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                                "text": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 25,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1,
                                    "end_line_num": 9
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1378,
                            "end_ix": 1390,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1390,
                            "end_ix": 1400,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                                "text": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 25,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1,
                                    "end_line_num": 9
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1460,
                            "end_ix": 1473,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Evaluating%20RAG%20systems%20involves%20considering,for%20RAG%20systems%20contributing%20in",
                                "text": "Evaluating RAG systems involves considering both the retrieval and generation components and the complexity of assessing overall system performance. The complexity of RAG systems is further compounded by the dynamic nature of external knowledge sources and the subjective nature of certain tasks, such as content creation or open domain question answering [RAGServey, Zhang2023c]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [Barnett2024, Cuconasu2024]. Then for this paper, we establish the comprehensive analysis framework for RAG systems contributing in",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 92,
                                    "end_line_num": 100
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1473,
                            "end_ix": 1483,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                                "text": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 25,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1,
                                    "end_line_num": 9
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1655,
                            "end_ix": 1667,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=for%20the%20RAG%20systems%2C%20identifying,and%20outlines%20future%20research%20directions",
                                "text": "for the RAG systems, identifying challenges associated with the retrieval and generation components and their integration within the overall system. Specifically, we propose an analysis framework for benchmarks of RAG systems, RGAR (R etrieval, G eneration, A dditional R equirement), which tackles the complexities inherent in RAG systems by focusing on measurable outputs and established truths. This paper offers a detailed methodology for systematically evaluating RAG systems and outlines future research directions.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 41,
                                    "end_line_num": 47
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1667,
                            "end_ix": 1680,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Based%20on%20these%20observations%2C%20we,comparing%20the%20various%20RAG%20benchmarks",
                                "text": "Based on these observations, we propose an analysis framework called RGAR (R etrieval, G eneration, and A dditional R equirement) for evaluation of RAG systems. In Figure 2, the framework consists of three components evaluation procedure, target, metrics, and datasets. It is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs between the “Evaluable Outputs” and ground truths. In the following sections, we will explain the framework in detail and utilize it for comparing the various RAG benchmarks.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 4,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 285,
                                    "end_line_num": 293
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1775,
                            "end_ix": 1788,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Based%20on%20these%20observations%2C%20we,comparing%20the%20various%20RAG%20benchmarks",
                                "text": "Based on these observations, we propose an analysis framework called RGAR (R etrieval, G eneration, and A dditional R equirement) for evaluation of RAG systems. In Figure 2, the framework consists of three components evaluation procedure, target, metrics, and datasets. It is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs between the “Evaluable Outputs” and ground truths. In the following sections, we will explain the framework in detail and utilize it for comparing the various RAG benchmarks.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 4,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 285,
                                    "end_line_num": 293
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1788,
                            "end_ix": 1802,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 363,
                                    "end_line_num": 366
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1829,
                            "end_ix": 1845,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "image_inline",
                                "asset_pointer_links": [
                                    "browse://fetch_multimodal_raw?url=https%3A%2F%2Far5iv.org%2Fhtml%2F2504.14891%2Fassets%2Fx1.png&zdr_type=3&check_internal_classifiers=False&original_conversation_id=681ba4a7-36e8-8004-a4be-67015f8f5c6c"
                                ],
                                "clicked_from_url": "https://ar5iv.org/html/2504.14891v1",
                                "clicked_from_title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey"
                            }
                        },
                        {
                            "start_ix": 1935,
                            "end_ix": 1947,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=There%20are%20many%20types%20of,to%20the%20user%E2%80%99s%20query%2C%20often",
                                "text": "There are many types of RAGs with different structures [RAGServey]. Figure 1 illustrates the structure of the RAG systems we used in this paper, which concentrated on two functional components and four phases. The retrieval component is tasked with extracting relevant information from a vast array of external knowledge sources. This process involves two main phases: indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [RAGServey, FAISS, ColBERT]. The search phase then utilizes these indexes to fetch documents most relevant to the user’s query, often",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 67,
                                    "end_line_num": 75
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1947,
                            "end_ix": 1961,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                                "text": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 172,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1988,
                            "end_ix": 2002,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                                "text": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 172,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2023,
                            "end_ix": 2037,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                                "text": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 172,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2076,
                            "end_ix": 2090,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                                "text": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 177,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2090,
                            "end_ix": 2102,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=The%20generation%20phase%20follows%2C%20utilizing,Appendix%200.A%20provides%20a",
                                "text": "The generation phase follows, utilizing the retrieved information to formulate coherent and contextually relevant responses. This phase comprises prompting and inference operations. Prompting strategically formulates input for the language model, integrating the query with findings from the retrieval phase, using methods such as Chain of Thought (CoT) [CoT] or Rephrase and Respond (RaR) [RaR] to guide better generation results. Inference then involves Large Language Models (LLMs) interpreting the prompted input to generate textual responses that align with the query’s intent and reflect the accuracy and depth of the extracted information [Lewis2021, Devlin2019]. Appendix 0.A provides a",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 80,
                                    "end_line_num": 88
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2149,
                            "end_ix": 2163,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                                "text": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 71,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 177,
                                    "end_line_num": 180
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2163,
                            "end_ix": 2175,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                                "text": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 69,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2405,
                            "end_ix": 2419,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                                "text": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 249,
                                    "end_line_num": 257
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2482,
                            "end_ix": 2496,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                                "text": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 249,
                                    "end_line_num": 257
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2540,
                            "end_ix": 2554,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                                "text": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 249,
                                    "end_line_num": 257
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2554,
                            "end_ix": 2568,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Coverage%20measures%20the%20proportion%20of,or%20in%20the%20knowledge%20base",
                                "text": "Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents available in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 403,
                                    "end_line_num": 411
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2628,
                            "end_ix": 2642,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                                "text": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 249,
                                    "end_line_num": 257
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2642,
                            "end_ix": 2656,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Coverage%20measures%20the%20proportion%20of,or%20in%20the%20knowledge%20base",
                                "text": "Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents available in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 403,
                                    "end_line_num": 411
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2687,
                            "end_ix": 2701,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Coverage%20measures%20the%20proportion%20of,or%20in%20the%20knowledge%20base",
                                "text": "Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents available in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 403,
                                    "end_line_num": 411
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2941,
                            "end_ix": 2955,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Relevance%20,meets%20the%20query%E2%80%99s%20specific%20requirements",
                                "text": "Relevance (Response Query): This aspect measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query’s specific requirements.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 334,
                                    "end_line_num": 344
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2955,
                            "end_ix": 2969,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Correctness%20,the%20context%20of%20the%20query",
                                "text": "Correctness (Response Sample Response): Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 348,
                                    "end_line_num": 352
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2969,
                            "end_ix": 2983,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                                "text": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 368,
                                    "end_line_num": 374
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3082,
                            "end_ix": 3096,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Relevance%20,meets%20the%20query%E2%80%99s%20specific%20requirements",
                                "text": "Relevance (Response Query): This aspect measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query’s specific requirements.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 334,
                                    "end_line_num": 344
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3096,
                            "end_ix": 3110,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Correctness%20,the%20context%20of%20the%20query",
                                "text": "Correctness (Response Sample Response): Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 348,
                                    "end_line_num": 352
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3165,
                            "end_ix": 3179,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Correctness%20,the%20context%20of%20the%20query",
                                "text": "Correctness (Response Sample Response): Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 348,
                                    "end_line_num": 352
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3230,
                            "end_ix": 3244,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 339,
                                    "end_line_num": 344
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3244,
                            "end_ix": 3256,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                                "text": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 28,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 37,
                                    "end_line_num": 40
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3301,
                            "end_ix": 3315,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 339,
                                    "end_line_num": 344
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3315,
                            "end_ix": 3327,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                                "text": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 28,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 37,
                                    "end_line_num": 40
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3509,
                            "end_ix": 3523,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                                "text": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 368,
                                    "end_line_num": 374
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3802,
                            "end_ix": 3816,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 354,
                                    "end_line_num": 360
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3816,
                            "end_ix": 3830,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 363,
                                    "end_line_num": 366
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3891,
                            "end_ix": 3905,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 363,
                                    "end_line_num": 366
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3944,
                            "end_ix": 3958,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                                "text": "*  -",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 354,
                                    "end_line_num": 360
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4033,
                            "end_ix": 4047,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                                "url": "https://arxiv.org/html/2405.07437v1#:~:text=Latency%3A%20This%20measures%20the%20time,for%20deployment%20and%20user%20experience",
                                "text": "Latency: This measures the time it takes for the system to retrieve information and generate a response, which is important for deployment and user experience.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 30,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 365,
                                    "end_line_num": 374
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4227,
                            "end_ix": 4241,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                                "text": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 368,
                                    "end_line_num": 374
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4241,
                            "end_ix": 4253,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                                "text": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 28,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 37,
                                    "end_line_num": 40
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4271,
                            "end_ix": 4285,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                                "text": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 368,
                                    "end_line_num": 374
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4561,
                            "end_ix": 4573,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                                "text": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 28,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 37,
                                    "end_line_num": 40
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 5050,
                            "end_ix": 5062,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Datasets,Recent%20benchmarks%20counter%20these%20limitations",
                                "text": "Datasets. We compiled the benchmarks along with the associated datasets in recent years. Early works focus on static general-purpose QA datasets (e.g., NQ [100], HotpotQA [101]), providing well-established baselines but lack recency or domain specificity. Recent benchmarks counter these limitations",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 61,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 55,
                                    "end_line_num": 62
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 5101,
                            "end_ix": 5117,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Comprehensiveness%20of%20the%20Evaluation%20Framework,methodologically%20but%20also%20linguistically%20diverse",
                                "text": "Comprehensiveness of the Evaluation Framework. Despite the abundant evaluation frameworks at present, individual ones are somewhat limited in their metrics and methods of evaluation. Moreover, most contemporary frameworks concentrate on widely used languages such as English and Chinese. There is an urgent need for frameworks that are not only methodologically but also linguistically diverse.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 60,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1089,
                                    "end_line_num": 1093
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 5117,
                            "end_ix": 5129,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Datasets,Recent%20benchmarks%20counter%20these%20limitations",
                                "text": "Datasets. We compiled the benchmarks along with the associated datasets in recent years. Early works focus on static general-purpose QA datasets (e.g., NQ [100], HotpotQA [101]), providing well-established baselines but lack recency or domain specificity. Recent benchmarks counter these limitations",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 61,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 55,
                                    "end_line_num": 62
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 5207,
                            "end_ix": 5223,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                                "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Advanced%20Evaluation%20Methods,and%20development%20in%20the%20future",
                                "text": "Advanced Evaluation Methods. As LLMs continue to evolve, the components of RAG systems are becoming more diverse. Currently, many of these components are evaluated using end-to-end RAG ontology metrics, with a lack of comprehensive functional decomposition evaluation or theoretical analysis. Concurrently, there remains untapped potential in the functionalities of LLMs themselves. For instance, the evaluation about deep thinking models (e.g. openai-o1 [135]) along with the thinking process of LLMs in conjunction with RAG’s retrieval and generation process, is still inadequate. These in-depth evaluation strategies require further research and development in the future.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 60,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1079,
                                    "end_line_num": 1088
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 5436,
                            "end_ix": 5448,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2501.03995] RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance",
                                "url": "https://arxiv.org/abs/2501.03995#:~:text=reliability%20of%20multi,performances%20using%20RS%20and%20CS",
                                "text": "reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response. We train RS and CS models using a ChatGPT-derived database and human evaluator samples. Results show that both models achieve ~88% accuracy on test data. Additionally, we construct a 5000-sample human-annotated database evaluating the relevancy of retrieved pieces and the correctness of response statements. Our RS model aligns with human preferences 20% more often than CLIP in retrieval, and our CS model matches human preferences ~91% of the time. Finally, we assess various RAG systems' selection and generation performances using RS and CS.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 63,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 61,
                                    "end_line_num": 70
                                },
                                "og_tags": null
                            }
                        }
                    ],
                    "content_references": [
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 95,
                            "end_idx": 107,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【24†L39-L45】",
                            "start_idx": 107,
                            "end_idx": 119,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                            "pub_date": null,
                            "snippet": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                            "attribution": "proceedings.neurips.cc",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【22†L60-L68】",
                            "start_idx": 156,
                            "end_idx": 168,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "RAG | Papers With Code",
                            "url": "https://paperswithcode.com/task/rag#:~:text=coherence%20of%20generated%20text%2C%20especially,MS%20MARCO%2C%20TriviaQA%2C%20and%20SQuAD",
                            "pub_date": null,
                            "snippet": "coherence of generated text, especially in tasks requiring detailed knowledge or long context handling. RAG is particularly useful in open-domain question answering, knowledge-grounded dialogue, and summarization tasks. The retrieval step helps the model to access and incorporate external information, making it less reliant on memorized knowledge and better suited for generating responses based on the latest or domain-specific information. The performance of RAG systems is usually measured using metrics such as precision, recall, F1 score, BLEU score, and exact match. Some popular datasets for evaluating RAG models include Natural Questions, MS MARCO, TriviaQA, and SQuAD.",
                            "attribution": "paperswithcode.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【24†L39-L45】",
                            "start_idx": 168,
                            "end_idx": 180,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                            "pub_date": null,
                            "snippet": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                            "attribution": "proceedings.neurips.cc",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 249,
                            "end_idx": 261,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【25†L1-L9】",
                            "start_idx": 261,
                            "end_idx": 271,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                            "pub_date": null,
                            "snippet": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 645,
                            "end_idx": 657,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【24†L39-L45】",
                            "start_idx": 657,
                            "end_idx": 669,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                            "pub_date": null,
                            "snippet": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                            "attribution": "proceedings.neurips.cc",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【24†L39-L45】",
                            "start_idx": 767,
                            "end_idx": 779,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "url": "https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html#:~:text=whole%20generated%20sequence%2C%20the%20other,only%20seq2seq%20baseline",
                            "pub_date": null,
                            "snippet": "whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                            "attribution": "proceedings.neurips.cc",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【46†L66-L70】",
                            "start_idx": 859,
                            "end_idx": 871,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "",
                            "url": "https://aclanthology.org/2024.emnlp-industry.108.pdf#:~:text=open,for%20answering%20questions%20in%20conversational",
                            "pub_date": null,
                            "snippet": "open-domain QA tasks. These models, referred to as retrieval-augmented generation (RAG) mod\u0002els, offer superior performance and flexibility (Raf\u0002fel et al., 2020; Min et al., 2020; Lewis et al., 2020b). Moreover, generative models are well\u0002suited for answering questions in conversational",
                            "attribution": "aclanthology.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【45†L67-L75】",
                            "start_idx": 964,
                            "end_idx": 976,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=There%20are%20many%20types%20of,to%20the%20user%E2%80%99s%20query%2C%20often",
                            "pub_date": null,
                            "snippet": "There are many types of RAGs with different structures [RAGServey]. Figure 1 illustrates the structure of the RAG systems we used in this paper, which concentrated on two functional components and four phases. The retrieval component is tasked with extracting relevant information from a vast array of external knowledge sources. This process involves two main phases: indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [RAGServey, FAISS, ColBERT]. The search phase then utilizes these indexes to fetch documents most relevant to the user’s query, often",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L172-L180】",
                            "start_idx": 976,
                            "end_idx": 990,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【45†L67-L75】",
                            "start_idx": 1072,
                            "end_idx": 1084,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=There%20are%20many%20types%20of,to%20the%20user%E2%80%99s%20query%2C%20often",
                            "pub_date": null,
                            "snippet": "There are many types of RAGs with different structures [RAGServey]. Figure 1 illustrates the structure of the RAG systems we used in this paper, which concentrated on two functional components and four phases. The retrieval component is tasked with extracting relevant information from a vast array of external knowledge sources. This process involves two main phases: indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [RAGServey, FAISS, ColBERT]. The search phase then utilizes these indexes to fetch documents most relevant to the user’s query, often",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L177-L180】",
                            "start_idx": 1084,
                            "end_idx": 1098,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L177-L180】",
                            "start_idx": 1154,
                            "end_idx": 1168,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 1168,
                            "end_idx": 1180,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 1222,
                            "end_idx": 1234,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【25†L1-L9】",
                            "start_idx": 1234,
                            "end_idx": 1244,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                            "pub_date": null,
                            "snippet": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 1378,
                            "end_idx": 1390,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【25†L1-L9】",
                            "start_idx": 1390,
                            "end_idx": 1400,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                            "pub_date": null,
                            "snippet": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【45†L92-L100】",
                            "start_idx": 1460,
                            "end_idx": 1473,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Evaluating%20RAG%20systems%20involves%20considering,for%20RAG%20systems%20contributing%20in",
                            "pub_date": null,
                            "snippet": "Evaluating RAG systems involves considering both the retrieval and generation components and the complexity of assessing overall system performance. The complexity of RAG systems is further compounded by the dynamic nature of external knowledge sources and the subjective nature of certain tasks, such as content creation or open domain question answering [RAGServey, Zhang2023c]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [Barnett2024, Cuconasu2024]. Then for this paper, we establish the comprehensive analysis framework for RAG systems contributing in",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【25†L1-L9】",
                            "start_idx": 1473,
                            "end_idx": 1483,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=presents%20unique%20challenges%20due%20to,evaluation%20methods%20and%20frameworks%2C%20systematically",
                            "pub_date": null,
                            "snippet": "presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L41-L47】",
                            "start_idx": 1655,
                            "end_idx": 1667,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=for%20the%20RAG%20systems%2C%20identifying,and%20outlines%20future%20research%20directions",
                            "pub_date": null,
                            "snippet": "for the RAG systems, identifying challenges associated with the retrieval and generation components and their integration within the overall system. Specifically, we propose an analysis framework for benchmarks of RAG systems, RGAR (R etrieval, G eneration, A dditional R equirement), which tackles the complexities inherent in RAG systems by focusing on measurable outputs and established truths. This paper offers a detailed methodology for systematically evaluating RAG systems and outlines future research directions.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【4†L285-L293】",
                            "start_idx": 1667,
                            "end_idx": 1680,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Based%20on%20these%20observations%2C%20we,comparing%20the%20various%20RAG%20benchmarks",
                            "pub_date": null,
                            "snippet": "Based on these observations, we propose an analysis framework called RGAR (R etrieval, G eneration, and A dditional R equirement) for evaluation of RAG systems. In Figure 2, the framework consists of three components evaluation procedure, target, metrics, and datasets. It is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs between the “Evaluable Outputs” and ground truths. In the following sections, we will explain the framework in detail and utilize it for comparing the various RAG benchmarks.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【4†L285-L293】",
                            "start_idx": 1775,
                            "end_idx": 1788,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Based%20on%20these%20observations%2C%20we,comparing%20the%20various%20RAG%20benchmarks",
                            "pub_date": null,
                            "snippet": "Based on these observations, we propose an analysis framework called RGAR (R etrieval, G eneration, and A dditional R equirement) for evaluation of RAG systems. In Figure 2, the framework consists of three components evaluation procedure, target, metrics, and datasets. It is designed to provide a systematic methodology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs between the “Evaluable Outputs” and ground truths. In the following sections, we will explain the framework in detail and utilize it for comparing the various RAG benchmarks.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L363-L366】",
                            "start_idx": 1788,
                            "end_idx": 1802,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【37†embed_image】",
                            "start_idx": 1829,
                            "end_idx": 1845,
                            "alt": null,
                            "type": "image_inline",
                            "asset_pointer_links": [
                                "browse://fetch_multimodal_raw?url=https%3A%2F%2Far5iv.org%2Fhtml%2F2504.14891%2Fassets%2Fx1.png&zdr_type=3&check_internal_classifiers=False&original_conversation_id=681ba4a7-36e8-8004-a4be-67015f8f5c6c"
                            ],
                            "clicked_from_url": "https://ar5iv.org/html/2504.14891v1",
                            "clicked_from_title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey"
                        },
                        {
                            "matched_text": "【45†L67-L75】",
                            "start_idx": 1935,
                            "end_idx": 1947,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=There%20are%20many%20types%20of,to%20the%20user%E2%80%99s%20query%2C%20often",
                            "pub_date": null,
                            "snippet": "There are many types of RAGs with different structures [RAGServey]. Figure 1 illustrates the structure of the RAG systems we used in this paper, which concentrated on two functional components and four phases. The retrieval component is tasked with extracting relevant information from a vast array of external knowledge sources. This process involves two main phases: indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [RAGServey, FAISS, ColBERT]. The search phase then utilizes these indexes to fetch documents most relevant to the user’s query, often",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L172-L180】",
                            "start_idx": 1947,
                            "end_idx": 1961,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L172-L180】",
                            "start_idx": 1988,
                            "end_idx": 2002,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L172-L180】",
                            "start_idx": 2023,
                            "end_idx": 2037,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=strategies%20can%20be%20employed%2C%20including,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods[6 ,  43]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [40 ,  6]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L177-L180】",
                            "start_idx": 2076,
                            "end_idx": 2090,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【45†L80-L88】",
                            "start_idx": 2090,
                            "end_idx": 2102,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=The%20generation%20phase%20follows%2C%20utilizing,Appendix%200.A%20provides%20a",
                            "pub_date": null,
                            "snippet": "The generation phase follows, utilizing the retrieved information to formulate coherent and contextually relevant responses. This phase comprises prompting and inference operations. Prompting strategically formulates input for the language model, integrating the query with findings from the retrieval phase, using methods such as Chain of Thought (CoT) [CoT] or Rephrase and Respond (RaR) [RaR] to guide better generation results. Inference then involves Large Language Models (LLMs) interpreting the prompted input to generate textual responses that align with the query’s intent and reflect the accuracy and depth of the extracted information [Lewis2021, Devlin2019]. Appendix 0.A provides a",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【71†L177-L180】",
                            "start_idx": 2149,
                            "end_idx": 2163,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=generation%20pipeline%2C%20the%20responding%20progress,thereby%20reducing%20the%20risk%20of",
                            "pub_date": null,
                            "snippet": "generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision-maker or reasoner [8]. Instead of generating knowledge independently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【69†L58-L64】",
                            "start_idx": 2163,
                            "end_idx": 2175,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=facts,of%20more%20intelligent%20and%20versatile",
                            "pub_date": null,
                            "snippet": "facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [Huang2023] or factually incorrect generative outputs, thereby improving the content’s reliability and richness. [Zhang2023] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a pivotal development in the pursuit of more intelligent and versatile",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L249-L257】",
                            "start_idx": 2405,
                            "end_idx": 2419,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                            "pub_date": null,
                            "snippet": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L249-L257】",
                            "start_idx": 2482,
                            "end_idx": 2496,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                            "pub_date": null,
                            "snippet": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L249-L257】",
                            "start_idx": 2540,
                            "end_idx": 2554,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                            "pub_date": null,
                            "snippet": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L403-L411】",
                            "start_idx": 2554,
                            "end_idx": 2568,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Coverage%20measures%20the%20proportion%20of,or%20in%20the%20knowledge%20base",
                            "pub_date": null,
                            "snippet": "Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents available in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L249-L257】",
                            "start_idx": 2628,
                            "end_idx": 2642,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Relevance%20,specificity%20of%20the%20retrieval%20process",
                            "pub_date": null,
                            "snippet": "Relevance (Relevant Documents Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and specificity of the retrieval process.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L403-L411】",
                            "start_idx": 2642,
                            "end_idx": 2656,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Coverage%20measures%20the%20proportion%20of,or%20in%20the%20knowledge%20base",
                            "pub_date": null,
                            "snippet": "Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents available in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L403-L411】",
                            "start_idx": 2687,
                            "end_idx": 2701,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Coverage%20measures%20the%20proportion%20of,or%20in%20the%20knowledge%20base",
                            "pub_date": null,
                            "snippet": "Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents available in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L334-L344】",
                            "start_idx": 2941,
                            "end_idx": 2955,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Relevance%20,meets%20the%20query%E2%80%99s%20specific%20requirements",
                            "pub_date": null,
                            "snippet": "Relevance (Response Query): This aspect measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query’s specific requirements.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L348-L352】",
                            "start_idx": 2955,
                            "end_idx": 2969,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Correctness%20,the%20context%20of%20the%20query",
                            "pub_date": null,
                            "snippet": "Correctness (Response Sample Response): Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L368-L374】",
                            "start_idx": 2969,
                            "end_idx": 2983,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                            "pub_date": null,
                            "snippet": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L334-L344】",
                            "start_idx": 3082,
                            "end_idx": 3096,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Relevance%20,meets%20the%20query%E2%80%99s%20specific%20requirements",
                            "pub_date": null,
                            "snippet": "Relevance (Response Query): This aspect measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query’s specific requirements.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L348-L352】",
                            "start_idx": 3096,
                            "end_idx": 3110,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Correctness%20,the%20context%20of%20the%20query",
                            "pub_date": null,
                            "snippet": "Correctness (Response Sample Response): Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L348-L352】",
                            "start_idx": 3165,
                            "end_idx": 3179,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Correctness%20,the%20context%20of%20the%20query",
                            "pub_date": null,
                            "snippet": "Correctness (Response Sample Response): Similar to the accuracy in the retrieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L339-L344】",
                            "start_idx": 3230,
                            "end_idx": 3244,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【28†L37-L40】",
                            "start_idx": 3244,
                            "end_idx": 3256,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                            "pub_date": null,
                            "snippet": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L339-L344】",
                            "start_idx": 3301,
                            "end_idx": 3315,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【28†L37-L40】",
                            "start_idx": 3315,
                            "end_idx": 3327,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                            "pub_date": null,
                            "snippet": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L368-L374】",
                            "start_idx": 3509,
                            "end_idx": 3523,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                            "pub_date": null,
                            "snippet": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L354-L360】",
                            "start_idx": 3802,
                            "end_idx": 3816,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L363-L366】",
                            "start_idx": 3816,
                            "end_idx": 3830,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L363-L366】",
                            "start_idx": 3891,
                            "end_idx": 3905,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L354-L360】",
                            "start_idx": 3944,
                            "end_idx": 3958,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=%2A%20%20",
                            "pub_date": null,
                            "snippet": "*  -",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【30†L365-L374】",
                            "start_idx": 4033,
                            "end_idx": 4047,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                            "url": "https://arxiv.org/html/2405.07437v1#:~:text=Latency%3A%20This%20measures%20the%20time,for%20deployment%20and%20user%20experience",
                            "pub_date": null,
                            "snippet": "Latency: This measures the time it takes for the system to retrieve information and generate a response, which is important for deployment and user experience.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L368-L374】",
                            "start_idx": 4227,
                            "end_idx": 4241,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                            "pub_date": null,
                            "snippet": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【28†L37-L40】",
                            "start_idx": 4241,
                            "end_idx": 4253,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                            "pub_date": null,
                            "snippet": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L368-L374】",
                            "start_idx": 4271,
                            "end_idx": 4285,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=BLEU%20%28Bilingual%20Evaluation%20Understudy%29%20,fluency%20or%20grammaticality%20of%20the",
                            "pub_date": null,
                            "snippet": "BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the reference text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evaluation, BLEU can also be used for supervised comparison evaluation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or grammaticality of the",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【28†L37-L40】",
                            "start_idx": 4561,
                            "end_idx": 4573,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Factuality%20focuses%20on%20generating%20accurate,when%20the%20context",
                            "pub_date": null,
                            "snippet": "Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [83, 84, 78]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【61†L55-L62】",
                            "start_idx": 5050,
                            "end_idx": 5062,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Datasets,Recent%20benchmarks%20counter%20these%20limitations",
                            "pub_date": null,
                            "snippet": "Datasets. We compiled the benchmarks along with the associated datasets in recent years. Early works focus on static general-purpose QA datasets (e.g., NQ [100], HotpotQA [101]), providing well-established baselines but lack recency or domain specificity. Recent benchmarks counter these limitations",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【60†L1089-L1093】",
                            "start_idx": 5101,
                            "end_idx": 5117,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Comprehensiveness%20of%20the%20Evaluation%20Framework,methodologically%20but%20also%20linguistically%20diverse",
                            "pub_date": null,
                            "snippet": "Comprehensiveness of the Evaluation Framework. Despite the abundant evaluation frameworks at present, individual ones are somewhat limited in their metrics and methods of evaluation. Moreover, most contemporary frameworks concentrate on widely used languages such as English and Chinese. There is an urgent need for frameworks that are not only methodologically but also linguistically diverse.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【61†L55-L62】",
                            "start_idx": 5117,
                            "end_idx": 5129,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Datasets,Recent%20benchmarks%20counter%20these%20limitations",
                            "pub_date": null,
                            "snippet": "Datasets. We compiled the benchmarks along with the associated datasets in recent years. Early works focus on static general-purpose QA datasets (e.g., NQ [100], HotpotQA [101]), providing well-established baselines but lack recency or domain specificity. Recent benchmarks counter these limitations",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【60†L1079-L1088】",
                            "start_idx": 5207,
                            "end_idx": 5223,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2504.14891] Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
                            "url": "https://ar5iv.org/html/2504.14891v1#:~:text=Advanced%20Evaluation%20Methods,and%20development%20in%20the%20future",
                            "pub_date": null,
                            "snippet": "Advanced Evaluation Methods. As LLMs continue to evolve, the components of RAG systems are becoming more diverse. Currently, many of these components are evaluated using end-to-end RAG ontology metrics, with a lack of comprehensive functional decomposition evaluation or theoretical analysis. Concurrently, there remains untapped potential in the functionalities of LLMs themselves. For instance, the evaluation about deep thinking models (e.g. openai-o1 [135]) along with the thinking process of LLMs in conjunction with RAG’s retrieval and generation process, is still inadequate. These in-depth evaluation strategies require further research and development in the future.",
                            "attribution": "ar5iv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【63†L61-L70】",
                            "start_idx": 5436,
                            "end_idx": 5448,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2501.03995] RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance",
                            "url": "https://arxiv.org/abs/2501.03995#:~:text=reliability%20of%20multi,performances%20using%20RS%20and%20CS",
                            "pub_date": null,
                            "snippet": "reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response. We train RS and CS models using a ChatGPT-derived database and human evaluator samples. Results show that both models achieve ~88% accuracy on test data. Additionally, we construct a 5000-sample human-annotated database evaluating the relevancy of retrieved pieces and the correctness of response statements. Our RS model aligns with human preferences 20% more often than CLIP in retrieval, and our CS model matches human preferences ~91% of the time. Finally, we assess various RAG systems' selection and generation performances using RS and CS.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        }
                    ],
                    "timestamp_": "absolute",
                    "message_type": null,
                    "is_async_task_result_message": true,
                    "b1de6e2_rm": true,
                    "async_task_id": "deepresch_681ba4eb92bc8191949c31b27d216ba4",
                    "async_task_title": "检索增强生成（RAG）系统评估的研究报告",
                    "model_switcher_deny": [
                        {
                            "slug": "o1-mini",
                            "context": "regenerate",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        },
                        {
                            "slug": "o1-mini",
                            "context": "conversation",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        }
                    ]
                },
                "recipient": "all",
                "channel": "final"
            },
            "parent": "1d9fe8d3-bbfd-4709-8240-fc031612f651",
            "children": []
        }
    },
    "moderation_results": [],
    "current_node": "9895a557-a971-404b-99c0-5d9986fdc70f",
    "plugin_ids": null,
    "conversation_id": "681ba4a7-36e8-8004-a4be-67015f8f5c6c",
    "conversation_template_id": null,
    "gizmo_id": null,
    "gizmo_type": null,
    "is_archived": false,
    "is_starred": null,
    "safe_urls": [],
    "blocked_urls": [],
    "default_model_slug": "research",
    "conversation_origin": null,
    "voice": null,
    "async_status": null,
    "disabled_tool_ids": [],
    "is_do_not_remember": null,
    "memory_scope": "global_enabled"
}
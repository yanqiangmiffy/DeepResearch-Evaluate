好的，我将为你撰写一份中文文献综述，梳理近两三年来大语言模型（LLM）在以下四个方面的重要研究进展：

1. **预训练**：探讨训练高性能 LLM 所采用的模型架构、训练数据构建策略、扩展性技术（如稀疏激活、混合专家模型）等。
2. **适配微调**：从有效性与安全性角度出发，综述如指令微调（Instruction Tuning）、RLHF、人类反馈学习、对抗训练、对齐技术等。
3. **使用**：分析 LLM 在多种下游任务（如代码生成、学术搜索、多模态问答、教育、医疗等）的应用方式与框架。
4. **能力评估**：汇总最近的评估基准（如 MMLU、BIG-Bench、HELM、Gaokao Bench 等），以及研究中关于 LLM 推理能力、幻觉、偏见、安全性的经验性发现。

我会整理近期（2022–2025）中英文文献中的核心成果和代表性论文，并以结构化方式呈现给你。稍后我会将综述内容发送给你。


# 大语言模型研究进展（2022–2025）

## 预训练

* **模型架构与扩展**：近年来，Transformer仍是主流架构，但为提升容量和计算效率，引入了稀疏激活技术等可扩展方法。例如，多专家(Mixture-of-Experts, MoE)结构通过只激活部分专家来增加模型参数量而不显著增加计算成本；Meta提出的Branch-Train-MiX方法则先将一个种子模型分支为多个领域专家并并行训练，随后合并成MoE层并学习路由，使得不同领域能力融合，提升了训练效率和多领域表现。此外，还有研究通过激活截断（如Q-Sparse）将中间层输出稀疏化，仅保留Top-K激活，从而显著降低推理开销同时保持接近的性能。
* **训练目标与多任务**：预训练通常采用自回归的下一个token预测目标，但也有研究将多任务或指令式数据纳入训练。比如Instruction Pre-Training方法在预训练阶段混合了大量「指令—响应」对，使模型在学习语言建模的同时习得执行用户指令的能力。这种多任务预训练方式帮助模型获得更广泛的知识和更强的泛化能力。
* **训练数据构建**：预训练语料量极为庞大，既包含网页爬取的大规模原始文本，也融合高质量的结构化数据（如百科、论文、开源书籍、编程代码等）。公开资料显示，业界最新开源语料集规模空前：如RedPajama-Data-v2从84份CommonCrawl爬取中筛选出约1000亿文档，去重后形成约30万亿(30T)标记的训练集；Meta的LLaMA 2在此之前仅使用了约2.4万亿个精心过滤的标记。数据预处理流程也十分复杂，包括清洗HTML、去除重复文本、语言识别过滤和敏感信息剔除等。例如在Dolma语料中，对代码仓库导出的文本进行隐私过滤，去除了提交记录中的个人邮箱等PII信息。
* **扩展技术**：为支持超大规模预训练，采用了多种高效训练技术。例如采用稀疏并行策略（如分布式训练、流水线并行）降低通信开销；在模型端则使用混合精度、参数分解（如LoRA）等手段加速训练。此外，MoE和激活稀疏化等结构创新也使得模型容量和推理效率得到平衡。

## 适配与微调

* **指令微调与RLHF**：为了使预训练模型更好地服从用户需求并提升安全性，研究者普遍采用了监督指令微调和基于人类反馈的强化学习(RLHF)。OpenAI的InstructGPT工作就是典型代表：首先收集标注者编写的问答示例对模型进行监督微调，然后再采集标注者对模型输出的偏好排序，使用偏好数据进行RLHF训练。结果表明，经过这种两阶段微调的1.3B模型输出结果优于未经对齐的175B GPT-3，在输出真实性和有害性方面都有显著改善。该方法奠定了指令调优+RLHF的标准流程。类似地，Anthropic提出的“宪法AI”方法(RLAIF)利用人工制定的一系列原则让模型自我批评和自我修正，无需额外人类标注，即可训练出更“无害”的助手模型。
* **对齐方法与人类反馈**：除了传统RLHF，还有研究探索其它对齐策略，如通过规则列表和模型自评进行对齐（即不直接使用人类偏好标签）。Constitutional AI方法在监督阶段让模型生成回答后依据原则进行自我批评修正，然后在强化学习阶段用模型偏好来训练奖励函数，从而学习以无害且有用的方式回应用户。这些方法能够在不需要大量人工评价数据的前提下，更精确地控制模型行为。
* **多轮对话增强**：针对对话场景，微调时会使用多轮对话数据来训练模型维持对话历史和上下文一致。例如，通过在训练示例中添加对话历史，模型学会在生成回答时考虑前文，表现出更连贯的多轮交互能力。实证发现，多轮微调使模型更适合客服、技术支持等需要连续对话的场景，能够自动学习在多轮查询中保持一致的风格和逻辑。
* **对抗训练与安全防护**：为了提高模型对恶意攻击或极端提示的鲁棒性，研究者引入了对抗训练策略。最新成果表明，大规模模型容易受到“越狱”(jailbreak)攻击而生成有害内容。为此，提出了基于内部激活特征的对抗训练算法。例如ReFAT方法通过模拟将“拒绝特征”(refusal feature)置为不活跃的最坏情况输入扰动，来增强模型安全性。实验表明，这种模拟的对抗训练在大幅提升模型对常见攻击的抵抗力时，计算开销远低于传统对抗训练。
* **隐私保护与数据脱敏**：在微调过程中，尤其是使用敏感领域数据时，需要关注隐私泄露风险。有文献综述指出，微调阶段可能面临成员推断、数据提取和后门注入等隐私攻击，需要采取差分隐私、联邦学习或知识遗忘等防御措施。此外，对训练语料的严格去标识化和脱敏处理（如使用正则表达式或NER技术剔除姓名、地址等PII）也被广泛应用于开源数据管道，以减少模型记忆和泄露私人信息的风险。

## 使用

* **教育应用**：在教育领域，LLM被用作个性化学习和智能辅导工具。研究指出，具有长期记忆和工具使用能力的LLM代理可以记录学生的学习习惯并访问外部资源，从而生成针对性的学习内容和习题，同时自动批改作业等，从而实现自适应教学。这些智能教育助手能根据学生的反馈动态调整教学策略，提高了学习的参与度和效率。
* **医疗场景**：LLM在医疗健康领域的使用主要集中于医学问答、病患教育材料生成、临床笔记撰写辅助等任务。一项系统性综述显示，大量研究在使用GPT-3.5/4模型回答病患提问、摘要或翻译医学文献，以及生成病历记录等方面取得了一定成果。例如，GPT系列模型可帮助患者理解疾病和治疗方案，提升医学信息的可访问性。然而，综述也强调了局限：许多模型尚未针对医疗领域优化，输出时常出现不准确、片面或偏见的内容，存在安全和合规风险。
* **代码生成**：在软件开发中，LLM被用于自动代码生成和辅助编程。知名应用包括基于OpenAI Codex的GitHub Copilot以及中国的CodeGeeX等。相关综述指出，大模型在代码补全、代码翻译与重构等任务上表现出色。在HumanEval、MBPP等基准测试中，现代LLM的代码生成能力持续提升，能根据自然语言提示生成符合规范的代码。
* **问答与推理系统**：LLM广泛用于构建开放域问答系统。链式思维(chain-of-thought)提示技术已经证明能够显著提高模型在复杂推理题上的性能：例如给出少量带有推理过程示例的提示后，一个540B参数模型在数学题集GSM8K上甚至超过了微调后的GPT-3。此外，检索增强生成(RAG)方法将模型与外部知识库结合，以解决LLM的时效性和幻觉问题。最新综述指出，通过引入外部检索，RAG框架可以提高知识密集型问答的准确性和可解释性。
* **多模态融合**：新一代LLM具备跨模态处理能力，能够同时理解文本、图像（甚至音频）输入。例如，OpenAI的GPT-4可以接受带图像的提示，并在文本基础上完成视觉任务；Google的PaLM-E、Meta的Emu等模型也在图像描述、视觉问答、机器人指令等多模态场景中表现出色。多模态能力使LLM在图像分析、医疗影像诊断等领域展现了广阔前景。
* **工具与插件**：为了进一步扩展LLM能力，还开发了调用工具和插件的技术。Toolformer提出了一种自监督方法，使模型自动学习在何时调用计算器、搜索引擎、问答系统等API，以及如何结合工具返回的结果进行后续生成。实验证明，这种方法显著提升了模型在多种任务上的零样本表现，而无需大幅增加模型规模。类似地，现有聊天机器人（如ChatGPT）的插件系统、检索工具和算术模块等使LLM在现实应用中更灵活可靠。

## 能力评估

* **评估基准**：社区提出了多种专门的基准来全面评估LLM各方面能力。如英语多学科知识测评MMLU，以及更难的BIG-Bench Hard任务；中国方向开发了GAOKAO-Bench，直接采用全国高考真题来评估模型在语文、数学、英语等学科的综合表现。HOleristic Evaluation of Language Models (HELM)等评价框架则设计了涵盖准确性、鲁棒性、公平性、偏见、毒性和效率等多维度指标，对几十种模型在几十个任务上进行交叉评估。
* **能力维度**：常见的评估维度包括**知识量**（事实性问答、百科知识等）、**推理能力**（数学题、逻辑题等）、**对话质量**（多轮对话一致性）、**多语言表现**（多语种理解与生成）以及**鲁棒性/安全性**（对抗样本、偏见与有害内容生成程度）等。例如，BIG-Bench的结果显示，随着模型规模增加，知识记忆和简单推理题性能逐步提升，但多步逻辑推理任务往往需要非常大的模型才出现突破性进展；而在具有歧义的社会文化问题上，随着规模增长模型偏见反而可能加剧。
* **实证发现**：大量评估揭示了当前LLM的优缺点。大型模型在广泛知识检索和语言生成上表现优异，但仍频繁产生“幻觉”——即输出不实或无根据的内容。研究指出，尽管通过指令微调和RLHF可部分提高输出真实性，但完全消除虚假信息仍具有挑战性。此外，模型在训练分布外的鲁棒性较弱，对抗样本和人为构造的恶意提示往往导致失败。社会偏见也是一大隐患：评估结果显示，LLM可能在性别、种族、文化等问题上继承或放大数据中的偏见，需要专门的公平性调研和缓解方法。这些实证发现提醒研究者，尽管LLM能力快速提升，但安全、可靠和公平使用仍需深入研究和严格把关。

**参考文献**：上述观点引用了近两年相关领域的重要文献与技术报告，以确保内容的准确性和时效性等。

{
    "title": "LLM进展综述",
    "create_time": 1746639684.242891,
    "update_time": 1746640751.124794,
    "mapping": {
        "client-created-root": {
            "id": "client-created-root",
            "message": null,
            "parent": null,
            "children": [
                "20198ea4-df02-45bf-8790-3250460492fc"
            ]
        },
        "20198ea4-df02-45bf-8790-3250460492fc": {
            "id": "20198ea4-df02-45bf-8790-3250460492fc",
            "message": {
                "id": "20198ea4-df02-45bf-8790-3250460492fc",
                "author": {
                    "role": "system",
                    "name": null,
                    "metadata": {}
                },
                "create_time": null,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        ""
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 0.0,
                "metadata": {
                    "is_visually_hidden_from_conversation": true
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "client-created-root",
            "children": [
                "1fafae9f-1237-4b4b-9ade-e21cad0581c1"
            ]
        },
        "1fafae9f-1237-4b4b-9ade-e21cad0581c1": {
            "id": "1fafae9f-1237-4b4b-9ade-e21cad0581c1",
            "message": {
                "id": "1fafae9f-1237-4b4b-9ade-e21cad0581c1",
                "author": {
                    "role": "user",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746639683.544,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "请从四个主要方面对 LLM 的最近进展进行文献综述，包括预训练（如何预训练 出一个有能力的 LLM）、适配微调（如何从有效性和安全性两个角度有效地微调预训练的 LLM）、使用（如何利用LLM 解决各种下游任务）以及能力评估（如何评估 LLM 的能力和现有的经验性发现）。"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": null,
                "weight": 1.0,
                "metadata": {
                    "selected_github_repos": [],
                    "system_hints": [
                        "research"
                    ],
                    "serialization_metadata": {
                        "custom_symbol_offsets": []
                    },
                    "request_id": "93c282074f007368-DFW",
                    "message_source": null,
                    "timestamp_": "absolute",
                    "message_type": null
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "20198ea4-df02-45bf-8790-3250460492fc",
            "children": [
                "97fc39f6-1675-4300-8553-f2f97656c9f9"
            ]
        },
        "97fc39f6-1675-4300-8553-f2f97656c9f9": {
            "id": "97fc39f6-1675-4300-8553-f2f97656c9f9",
            "message": {
                "id": "97fc39f6-1675-4300-8553-f2f97656c9f9",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746639686.258845,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "好的，我可以为你撰写关于大语言模型（LLM）最近进展的文献综述。为了确保内容符合你的需求，请问你希望涵盖的时间范围是近一年（2024年）内的研究，还是包括近两三年的重要进展？此外，你希望综述的语言为中文还是英文？"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 1.0,
                "metadata": {
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "1fafae9f-1237-4b4b-9ade-e21cad0581c1",
                    "request_id": "93c282074f007368-DFW",
                    "timestamp_": "absolute",
                    "model_switcher_deny": []
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "1fafae9f-1237-4b4b-9ade-e21cad0581c1",
            "children": [
                "13376f1b-8770-485d-80be-dff7aced487d"
            ]
        },
        "13376f1b-8770-485d-80be-dff7aced487d": {
            "id": "13376f1b-8770-485d-80be-dff7aced487d",
            "message": {
                "id": "13376f1b-8770-485d-80be-dff7aced487d",
                "author": {
                    "role": "tool",
                    "name": "research_kickoff_tool.clarify_with_text",
                    "metadata": {}
                },
                "create_time": 1746639686.258976,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        ""
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 1.0,
                "metadata": {
                    "is_visually_hidden_from_conversation": true,
                    "command": "clarify_with_text",
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "97fc39f6-1675-4300-8553-f2f97656c9f9",
                    "request_id": "93c282074f007368-DFW",
                    "timestamp_": "absolute"
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "97fc39f6-1675-4300-8553-f2f97656c9f9",
            "children": [
                "a1201d27-1376-418e-84f1-60bd03a3e130"
            ]
        },
        "a1201d27-1376-418e-84f1-60bd03a3e130": {
            "id": "a1201d27-1376-418e-84f1-60bd03a3e130",
            "message": {
                "id": "a1201d27-1376-418e-84f1-60bd03a3e130",
                "author": {
                    "role": "user",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746639700.969,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "近两三年的重要进展，中文"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": null,
                "weight": 1.0,
                "metadata": {
                    "selected_github_repos": [],
                    "system_hints": [
                        "research"
                    ],
                    "serialization_metadata": {
                        "custom_symbol_offsets": []
                    },
                    "request_id": "93c282765c067368-DFW",
                    "message_source": null,
                    "timestamp_": "absolute",
                    "message_type": null
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "13376f1b-8770-485d-80be-dff7aced487d",
            "children": [
                "24969176-b51f-4477-ad35-ff7bc9bac7da"
            ]
        },
        "24969176-b51f-4477-ad35-ff7bc9bac7da": {
            "id": "24969176-b51f-4477-ad35-ff7bc9bac7da",
            "message": {
                "id": "24969176-b51f-4477-ad35-ff7bc9bac7da",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746639713.827372,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "好的，我将为你撰写一份中文文献综述，梳理近两三年来大语言模型（LLM）在以下四个方面的重要研究进展：\n\n1. **预训练**：探讨训练高性能 LLM 所采用的模型架构、训练数据构建策略、扩展性技术（如稀疏激活、混合专家模型）等。\n2. **适配微调**：从有效性与安全性角度出发，综述如指令微调（Instruction Tuning）、RLHF、人类反馈学习、对抗训练、对齐技术等。\n3. **使用**：分析 LLM 在多种下游任务（如代码生成、学术搜索、多模态问答、教育、医疗等）的应用方式与框架。\n4. **能力评估**：汇总最近的评估基准（如 MMLU、BIG-Bench、HELM、Gaokao Bench 等），以及研究中关于 LLM 推理能力、幻觉、偏见、安全性的经验性发现。\n\n我会整理近期（2022–2025）中英文文献中的核心成果和代表性论文，并以结构化方式呈现给你。稍后我会将综述内容发送给你。"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": null,
                "weight": 1.0,
                "metadata": {
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "a1201d27-1376-418e-84f1-60bd03a3e130",
                    "request_id": "93c282765c067368-DFW",
                    "timestamp_": "absolute",
                    "model_switcher_deny": []
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "a1201d27-1376-418e-84f1-60bd03a3e130",
            "children": [
                "e9e64836-155b-486d-8f9e-6b1a65a812bd"
            ]
        },
        "e9e64836-155b-486d-8f9e-6b1a65a812bd": {
            "id": "e9e64836-155b-486d-8f9e-6b1a65a812bd",
            "message": {
                "id": "e9e64836-155b-486d-8f9e-6b1a65a812bd",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746639701.85969,
                "update_time": null,
                "content": {
                    "content_type": "code",
                    "language": "json",
                    "response_format_name": null,
                    "text": "{\n  \"task_violates_safety_guidelines\": false,\n  \"user_def_doesnt_want_research\": false,\n  \"response\": \"好的，我将为你撰写一份中文文献综述，梳理近两三年来大语言模型（LLM）在以下四个方面的重要研究进展：\\n\\n1. **预训练**：探讨训练高性能 LLM 所采用的模型架构、训练数据构建策略、扩展性技术（如稀疏激活、混合专家模型）等。\\n2. **适配微调**：从有效性与安全性角度出发，综述如指令微调（Instruction Tuning）、RLHF、人类反馈学习、对抗训练、对齐技术等。\\n3. **使用**：分析 LLM 在多种下游任务（如代码生成、学术搜索、多模态问答、教育、医疗等）的应用方式与框架。\\n4. **能力评估**：汇总最近的评估基准（如 MMLU、BIG-Bench、HELM、Gaokao Bench 等），以及研究中关于 LLM 推理能力、幻觉、偏见、安全性的经验性发现。\\n\\n我会整理近期（2022–2025）中英文文献中的核心成果和代表性论文，并以结构化方式呈现给你。稍后我会将综述内容发送给你。\",\n  \"title\": \"大语言模型的近期研究进展综述：预训练、微调、使用与评估\",\n  \"prompt\": \"请撰写一篇中文文献综述，梳理近两三年（2022年至2025年）大语言模型（LLM）在以下四个方面的研究进展：\\n\\n1. 预训练：涵盖如何预训练出一个有能力的 LLM，包括模型架构设计、训练目标、训练数据构建与选择、多任务学习、扩展性技术（如稀疏激活、混合专家）等。\\n\\n2. 适配微调：包括如何从有效性和安全性两个角度对预训练模型进行微调，涉及技术包括指令微调、RLHF、对齐方法、人类反馈、多轮对话增强、对抗训练、脱敏技术等。\\n\\n3. 使用：探讨 LLM 在各种下游任务中的应用方式与策略，如学术研究、教育、医疗、代码生成、问答系统、多模态融合任务等，包括使用链式思维（CoT）、工具增强（如工具调用、插件）等能力提升技术。\\n\\n4. 能力评估：如何评估 LLM 的能力，包括使用的评估基准（如 MMLU、BIG-Bench、Gaokao Bench、HELM 等）、能力维度（知识、推理、对话、多语言、鲁棒性、安全性等）及经验性发现（如幻觉率、偏见、鲁棒性问题、社会影响等）。\\n\\n请引用关键文献并分类整理每一部分的主要研究趋势、代表方法和重要成果。格式为结构化综述报告，使用中文撰写。\"\n}"
                },
                "status": "finished_successfully",
                "end_turn": false,
                "weight": 1.0,
                "metadata": {
                    "finish_details": {
                        "type": "stop",
                        "stop_tokens": [
                            200007
                        ]
                    },
                    "is_complete": true,
                    "citations": [],
                    "content_references": [],
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "24969176-b51f-4477-ad35-ff7bc9bac7da",
                    "request_id": "93c282765c067368-DFW",
                    "timestamp_": "absolute",
                    "model_switcher_deny": [
                        {
                            "slug": "o1-mini",
                            "context": "regenerate",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        },
                        {
                            "slug": "o1-mini",
                            "context": "conversation",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        }
                    ],
                    "is_loading_message": true
                },
                "recipient": "research_kickoff_tool.start_research_task",
                "channel": null
            },
            "parent": "24969176-b51f-4477-ad35-ff7bc9bac7da",
            "children": [
                "8b4f79e9-b11d-4d47-a362-0afae2c29e20"
            ]
        },
        "8b4f79e9-b11d-4d47-a362-0afae2c29e20": {
            "id": "8b4f79e9-b11d-4d47-a362-0afae2c29e20",
            "message": {
                "id": "8b4f79e9-b11d-4d47-a362-0afae2c29e20",
                "author": {
                    "role": "tool",
                    "name": "research_kickoff_tool.start_research_task",
                    "metadata": {}
                },
                "create_time": 1746639726.088317,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": []
                },
                "status": "finished_successfully",
                "end_turn": false,
                "weight": 1.0,
                "metadata": {
                    "async_task_title": "大语言模型的近期研究进展综述：预训练、微调、使用与评估",
                    "async_task_prompt": "请撰写一篇中文文献综述，梳理近两三年（2022年至2025年）大语言模型（LLM）在以下四个方面的研究进展：\n\n1. 预训练：涵盖如何预训练出一个有能力的 LLM，包括模型架构设计、训练目标、训练数据构建与选择、多任务学习、扩展性技术（如稀疏激活、混合专家）等。\n\n2. 适配微调：包括如何从有效性和安全性两个角度对预训练模型进行微调，涉及技术包括指令微调、RLHF、对齐方法、人类反馈、多轮对话增强、对抗训练、脱敏技术等。\n\n3. 使用：探讨 LLM 在各种下游任务中的应用方式与策略，如学术研究、教育、医疗、代码生成、问答系统、多模态融合任务等，包括使用链式思维（CoT）、工具增强（如工具调用、插件）等能力提升技术。\n\n4. 能力评估：如何评估 LLM 的能力，包括使用的评估基准（如 MMLU、BIG-Bench、Gaokao Bench、HELM 等）、能力维度（知识、推理、对话、多语言、鲁棒性、安全性等）及经验性发现（如幻觉率、偏见、鲁棒性问题、社会影响等）。\n\n请引用关键文献并分类整理每一部分的主要研究趋势、代表方法和重要成果。格式为结构化综述报告，使用中文撰写。",
                    "async_task_type": "research",
                    "async_task_status_messages": {
                        "initial": "大语言模型的近期研究进展综述：预训练、微调、使用与评估",
                        "completed_with_time": "Research completed in 9m",
                        "completed_no_time": "已完成研究",
                        "completed": "已研究的时间 ",
                        "error": "无法研究",
                        "cancelled": "研究已取消"
                    },
                    "b1de6e2_s": true,
                    "command": "start_research_task",
                    "message_type": null,
                    "model_slug": "research",
                    "default_model_slug": "auto",
                    "parent_id": "e9e64836-155b-486d-8f9e-6b1a65a812bd",
                    "request_id": "93c282765c067368-DFW",
                    "timestamp_": "absolute",
                    "async_task_id": "deepresch_681b9b6e4a6c8191a80f3cbf2aac7a8e",
                    "async_task_conversation_id": "681b9b6e-5390-8004-a7dc-534816972c55",
                    "async_task_created_at": "2025-05-07 17:42:07.064276+00:00",
                    "deep_research_version": "mini"
                },
                "recipient": "all",
                "channel": null
            },
            "parent": "e9e64836-155b-486d-8f9e-6b1a65a812bd",
            "children": [
                "d67f3621-b13c-4d72-ab10-f49532dbbe51"
            ]
        },
        "d67f3621-b13c-4d72-ab10-f49532dbbe51": {
            "id": "d67f3621-b13c-4d72-ab10-f49532dbbe51",
            "message": {
                "id": "d67f3621-b13c-4d72-ab10-f49532dbbe51",
                "author": {
                    "role": "assistant",
                    "name": null,
                    "metadata": {}
                },
                "create_time": 1746640280.936643,
                "update_time": null,
                "content": {
                    "content_type": "text",
                    "parts": [
                        "# 大语言模型研究进展（2022–2025）\n\n## 预训练  \n- **模型架构与扩展**：近年来，Transformer仍是主流架构，但为提升容量和计算效率，引入了稀疏激活技术等可扩展方法。例如，多专家(Mixture-of-Experts, MoE)结构通过只激活部分专家来增加模型参数量而不显著增加计算成本【1†L79-L87】；Meta提出的Branch-Train-MiX方法则先将一个种子模型分支为多个领域专家并并行训练，随后合并成MoE层并学习路由，使得不同领域能力融合，提升了训练效率和多领域表现【15†L52-L60】。此外，还有研究通过激活截断（如Q-Sparse）将中间层输出稀疏化，仅保留Top-K激活，从而显著降低推理开销同时保持接近的性能【17†L26-L34】。  \n- **训练目标与多任务**：预训练通常采用自回归的下一个token预测目标，但也有研究将多任务或指令式数据纳入训练。比如Instruction Pre-Training方法在预训练阶段混合了大量「指令—响应」对，使模型在学习语言建模的同时习得执行用户指令的能力【25†L251-L259】。这种多任务预训练方式帮助模型获得更广泛的知识和更强的泛化能力。  \n- **训练数据构建**：预训练语料量极为庞大，既包含网页爬取的大规模原始文本，也融合高质量的结构化数据（如百科、论文、开源书籍、编程代码等）。公开资料显示，业界最新开源语料集规模空前：如RedPajama-Data-v2从84份CommonCrawl爬取中筛选出约1000亿文档，去重后形成约30万亿(30T)标记的训练集【20†L50-L53】【23†L90-L93】；Meta的LLaMA 2在此之前仅使用了约2.4万亿个精心过滤的标记【22†L79-L82】【23†L90-L93】。数据预处理流程也十分复杂，包括清洗HTML、去除重复文本、语言识别过滤和敏感信息剔除等。例如在Dolma语料中，对代码仓库导出的文本进行隐私过滤，去除了提交记录中的个人邮箱等PII信息【23†L133-L142】。  \n- **扩展技术**：为支持超大规模预训练，采用了多种高效训练技术。例如采用稀疏并行策略（如分布式训练、流水线并行）降低通信开销【15†L75-L82】；在模型端则使用混合精度、参数分解（如LoRA）等手段加速训练。此外，MoE和激活稀疏化等结构创新也使得模型容量和推理效率得到平衡【1†L79-L87】【17†L26-L34】。\n\n## 适配与微调  \n- **指令微调与RLHF**：为了使预训练模型更好地服从用户需求并提升安全性，研究者普遍采用了监督指令微调和基于人类反馈的强化学习(RLHF)。OpenAI的InstructGPT工作就是典型代表：首先收集标注者编写的问答示例对模型进行监督微调，然后再采集标注者对模型输出的偏好排序，使用偏好数据进行RLHF训练【27†L61-L70】。结果表明，经过这种两阶段微调的1.3B模型输出结果优于未经对齐的175B GPT-3，在输出真实性和有害性方面都有显著改善【27†L65-L70】。该方法奠定了指令调优+RLHF的标准流程。类似地，Anthropic提出的“宪法AI”方法(RLAIF)利用人工制定的一系列原则让模型自我批评和自我修正，无需额外人类标注，即可训练出更“无害”的助手模型【32†L69-L77】。  \n- **对齐方法与人类反馈**：除了传统RLHF，还有研究探索其它对齐策略，如通过规则列表和模型自评进行对齐（即不直接使用人类偏好标签）。Constitutional AI方法在监督阶段让模型生成回答后依据原则进行自我批评修正，然后在强化学习阶段用模型偏好来训练奖励函数，从而学习以无害且有用的方式回应用户【32†L69-L77】。这些方法能够在不需要大量人工评价数据的前提下，更精确地控制模型行为。  \n- **多轮对话增强**：针对对话场景，微调时会使用多轮对话数据来训练模型维持对话历史和上下文一致。例如，通过在训练示例中添加对话历史，模型学会在生成回答时考虑前文，表现出更连贯的多轮交互能力【41†L62-L64】。实证发现，多轮微调使模型更适合客服、技术支持等需要连续对话的场景，能够自动学习在多轮查询中保持一致的风格和逻辑【41†L62-L64】【52†L88-L97】。  \n- **对抗训练与安全防护**：为了提高模型对恶意攻击或极端提示的鲁棒性，研究者引入了对抗训练策略。最新成果表明，大规模模型容易受到“越狱”(jailbreak)攻击而生成有害内容【37†L49-L57】。为此，提出了基于内部激活特征的对抗训练算法。例如ReFAT方法通过模拟将“拒绝特征”(refusal feature)置为不活跃的最坏情况输入扰动，来增强模型安全性。实验表明，这种模拟的对抗训练在大幅提升模型对常见攻击的抵抗力时，计算开销远低于传统对抗训练【37†L57-L62】。  \n- **隐私保护与数据脱敏**：在微调过程中，尤其是使用敏感领域数据时，需要关注隐私泄露风险。有文献综述指出，微调阶段可能面临成员推断、数据提取和后门注入等隐私攻击，需要采取差分隐私、联邦学习或知识遗忘等防御措施【39†L40-L49】。此外，对训练语料的严格去标识化和脱敏处理（如使用正则表达式或NER技术剔除姓名、地址等PII）也被广泛应用于开源数据管道，以减少模型记忆和泄露私人信息的风险【23†L133-L142】【39†L40-L49】。  \n\n## 使用  \n- **教育应用**：在教育领域，LLM被用作个性化学习和智能辅导工具。研究指出，具有长期记忆和工具使用能力的LLM代理可以记录学生的学习习惯并访问外部资源，从而生成针对性的学习内容和习题，同时自动批改作业等，从而实现自适应教学【52†L88-L97】。这些智能教育助手能根据学生的反馈动态调整教学策略，提高了学习的参与度和效率。  \n- **医疗场景**：LLM在医疗健康领域的使用主要集中于医学问答、病患教育材料生成、临床笔记撰写辅助等任务。一项系统性综述显示，大量研究在使用GPT-3.5/4模型回答病患提问、摘要或翻译医学文献，以及生成病历记录等方面取得了一定成果【54†L105-L113】。例如，GPT系列模型可帮助患者理解疾病和治疗方案，提升医学信息的可访问性【54†L124-L132】。然而，综述也强调了局限：许多模型尚未针对医疗领域优化，输出时常出现不准确、片面或偏见的内容，存在安全和合规风险【54†L105-L113】【54†L124-L132】。  \n- **代码生成**：在软件开发中，LLM被用于自动代码生成和辅助编程。知名应用包括基于OpenAI Codex的GitHub Copilot以及中国的CodeGeeX等【50†L99-L102】。相关综述指出，大模型在代码补全、代码翻译与重构等任务上表现出色【50†L85-L94】。在HumanEval、MBPP等基准测试中，现代LLM的代码生成能力持续提升，能根据自然语言提示生成符合规范的代码。  \n- **问答与推理系统**：LLM广泛用于构建开放域问答系统。链式思维(chain-of-thought)提示技术已经证明能够显著提高模型在复杂推理题上的性能：例如给出少量带有推理过程示例的提示后，一个540B参数模型在数学题集GSM8K上甚至超过了微调后的GPT-3【43†L52-L60】。此外，检索增强生成(RAG)方法将模型与外部知识库结合，以解决LLM的时效性和幻觉问题。最新综述指出，通过引入外部检索，RAG框架可以提高知识密集型问答的准确性和可解释性【48†L52-L60】。  \n- **多模态融合**：新一代LLM具备跨模态处理能力，能够同时理解文本、图像（甚至音频）输入。例如，OpenAI的GPT-4可以接受带图像的提示，并在文本基础上完成视觉任务【60†L1-L4】；Google的PaLM-E、Meta的Emu等模型也在图像描述、视觉问答、机器人指令等多模态场景中表现出色。多模态能力使LLM在图像分析、医疗影像诊断等领域展现了广阔前景。  \n- **工具与插件**：为了进一步扩展LLM能力，还开发了调用工具和插件的技术。Toolformer提出了一种自监督方法，使模型自动学习在何时调用计算器、搜索引擎、问答系统等API，以及如何结合工具返回的结果进行后续生成【45†L52-L61】。实验证明，这种方法显著提升了模型在多种任务上的零样本表现，而无需大幅增加模型规模。类似地，现有聊天机器人（如ChatGPT）的插件系统、检索工具和算术模块等使LLM在现实应用中更灵活可靠。  \n\n## 能力评估  \n- **评估基准**：社区提出了多种专门的基准来全面评估LLM各方面能力。如英语多学科知识测评MMLU，以及更难的BIG-Bench Hard任务；中国方向开发了GAOKAO-Bench，直接采用全国高考真题来评估模型在语文、数学、英语等学科的综合表现【63†L58-L61】。HOleristic Evaluation of Language Models (HELM)等评价框架则设计了涵盖准确性、鲁棒性、公平性、偏见、毒性和效率等多维度指标【65†L62-L71】，对几十种模型在几十个任务上进行交叉评估。  \n- **能力维度**：常见的评估维度包括**知识量**（事实性问答、百科知识等）、**推理能力**（数学题、逻辑题等）、**对话质量**（多轮对话一致性）、**多语言表现**（多语种理解与生成）以及**鲁棒性/安全性**（对抗样本、偏见与有害内容生成程度）等【65†L62-L71】【73†L189-L197】。例如，BIG-Bench的结果显示，随着模型规模增加，知识记忆和简单推理题性能逐步提升，但多步逻辑推理任务往往需要非常大的模型才出现突破性进展【73†L189-L197】；而在具有歧义的社会文化问题上，随着规模增长模型偏见反而可能加剧【73†L202-L206】。  \n- **实证发现**：大量评估揭示了当前LLM的优缺点。大型模型在广泛知识检索和语言生成上表现优异，但仍频繁产生“幻觉”——即输出不实或无根据的内容【75†L69-L77】。研究指出，尽管通过指令微调和RLHF可部分提高输出真实性【27†L65-L70】，但完全消除虚假信息仍具有挑战性【75†L69-L77】。此外，模型在训练分布外的鲁棒性较弱，对抗样本和人为构造的恶意提示往往导致失败【37†L49-L57】【37†L57-L62】。社会偏见也是一大隐患：评估结果显示，LLM可能在性别、种族、文化等问题上继承或放大数据中的偏见，需要专门的公平性调研和缓解方法【73†L202-L206】。这些实证发现提醒研究者，尽管LLM能力快速提升，但安全、可靠和公平使用仍需深入研究和严格把关。\n\n**参考文献**：上述观点引用了近两年相关领域的重要文献与技术报告，以确保内容的准确性和时效性【1†L79-L87】【15†L52-L60】【17†L26-L34】【20†L50-L53】【22†L79-L82】【23†L90-L93】【25†L251-L259】【27†L65-L70】【32†L69-L77】【37†L57-L62】【39†L40-L49】【41†L62-L64】【52†L88-L97】【54†L105-L113】【50†L85-L94】【50†L99-L102】【43†L52-L60】【48†L52-L60】【45†L52-L61】【60†L1-L4】【65†L62-L71】【63†L58-L61】【73†L189-L197】【75†L69-L77】等。"
                    ]
                },
                "status": "finished_successfully",
                "end_turn": true,
                "weight": 1.0,
                "metadata": {
                    "citations": [
                        {
                            "start_ix": 157,
                            "end_ix": 168,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "",
                                "url": "https://arxiv.org/pdf/2407.06204#:~:text=is%20based%20on%20a%20simple,corresponding%20surge%20in%20computational%20demands",
                                "text": "is based on a simple yet powerful idea: different parts of a model, known as experts, specialize in different tasks or aspects of the data. With this paradigm, only pertinent experts are engaged for a given input, keeping the com\u0002putational cost in check while still benefiting from a large pool of specialized knowledge. This scalable and flexible innovation has offered an effective approach for adhering to the scaling law, allowing for increased model capacity without a corresponding surge in computational demands.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 1,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 79,
                                    "end_line_num": 87
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 257,
                            "end_ix": 269,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
                                "url": "https://arxiv.org/html/2403.07816v1#:~:text=We%20investigate%20efficient%20methods%20for,Merge%20method%2C%20which",
                                "text": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 15,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 60
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 334,
                            "end_ix": 346,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated | OpenReview",
                                "url": "https://openreview.net/forum?id=cit3SNnZ6Q#:~:text=Abstract%3A%20We%20introduce%2C%20Q,Sparse%20is%20effective%20in%20different",
                                "text": "Abstract: We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 17,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 26,
                                    "end_line_num": 34
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 481,
                            "end_ix": 495,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
                                "url": "https://arxiv.org/html/2406.14491v1#:~:text=Except%20for%20the%20pre,training%20scenarios",
                                "text": "Except for the pre-training data, Instruction Pre-Training keeps all other pre- training settings the same as Vanilla Pre-Training: training with the next-token prediction objective (Radford et al., 2018) and computing loss on all tokens. We conduct both general pre-training from scratch and domain-adaptive continued pre-training to verify the effectiveness in different pre-training scenarios.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 25,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 251,
                                    "end_line_num": 259
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 692,
                            "end_ix": 704,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                                "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=,for%20further%20filtering%20and%20weighting",
                                "text": "# Today, we’re releasing a new version of the RedPajama dataset, with 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations that can be used for further filtering and weighting.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 20,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 50,
                                    "end_line_num": 53
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 704,
                            "end_ix": 716,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Curating Public Datasets for LLM Pretraining",
                                "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=content%29,v2%3A%20An%20open%20dataset",
                                "text": "content). Large open datasets such as RedPajama-Data-v2 (released late 2023) demonstrate these collection methods: it gathered over 100 billion web documents from 84 CommonCrawl snapshots, then filtered and deduplicated them to 30 billion documents (~30 trillion tokens) ( 6 RedPajama-Data-v2: An open dataset",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 23,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 90,
                                    "end_line_num": 93
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 751,
                            "end_ix": 763,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                                "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=A%20central%20ingredient%20to%20state,sources%20are%20the%20crawls%20made",
                                "text": "A central ingredient to state-of-the-art open LLMs like Llama, Mistral, Falcon, MPT, and the RedPajama models is the large amounts of high-quality data that these models are trained on. For example, Llama 2 is trained on 2.4 trillion carefully curated tokens. The most prominent data sources are the crawls made",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 22,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 79,
                                    "end_line_num": 82
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 763,
                            "end_ix": 775,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Curating Public Datasets for LLM Pretraining",
                                "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=content%29,v2%3A%20An%20open%20dataset",
                                "text": "content). Large open datasets such as RedPajama-Data-v2 (released late 2023) demonstrate these collection methods: it gathered over 100 billion web documents from 84 CommonCrawl snapshots, then filtered and deduplicated them to 30 billion documents (~30 trillion tokens) ( 6 RedPajama-Data-v2: An open dataset",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 23,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 90,
                                    "end_line_num": 93
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 868,
                            "end_ix": 882,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Curating Public Datasets for LLM Pretraining",
                                "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=Another%20critical%20filter%20is%20for,isn%E2%80%99t%20perfect%2C%20but%20it%20substantially",
                                "text": "Another critical filter is for personal or sensitive information (PII). To responsibly use public data, many pipelines now detect and remove PII like phone numbers, email addresses, physical addresses, or personal names in contexts that look private. This is often done with regex patterns (for phone numbers, SSNs, etc.) and Named Entity Recognition to catch person names. Some teams incorporate dedicated PII detection libraries or models (e.g. Microsoft Presidio or custom classifiers) to scrub out any confidential data. For instance, in the Dolma corpus, the portion derived from software repositories (commits, issue discussions) was processed with PII removal to exclude things like author emails in commit logs. Automatic PII filtering isn’t perfect, but it substantially",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 23,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 133,
                                    "end_line_num": 142
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 950,
                            "end_ix": 962,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
                                "url": "https://arxiv.org/html/2403.07816v1#:~:text=multilinguality%20%28Zhao%20et%C2%A0al,41%3B%20Gemini%20Team%2C%202023",
                                "text": "multilinguality (Zhao et al., 2024 ), etc. Training such LLMs requires a large amount of compute and data, exceeding thousands of GPUs and trillions of tokens. The training parallelization is typically done by maintaining multiple copies of the model on different GPUs and keeping them synchronized after each weight update. The cost of this frequent communication is the main bottleneck in scaling the training to more GPUs. Besides this issue, synchronized training is more vulnerable to hardware failures as a single failed GPU can cause the whole training to halt (Zhang et al.,  41; Gemini Team, 2023).",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 15,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 75,
                                    "end_line_num": 82
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1027,
                            "end_ix": 1038,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "",
                                "url": "https://arxiv.org/pdf/2407.06204#:~:text=is%20based%20on%20a%20simple,corresponding%20surge%20in%20computational%20demands",
                                "text": "is based on a simple yet powerful idea: different parts of a model, known as experts, specialize in different tasks or aspects of the data. With this paradigm, only pertinent experts are engaged for a given input, keeping the com\u0002putational cost in check while still benefiting from a large pool of specialized knowledge. This scalable and flexible innovation has offered an effective approach for adhering to the scaling law, allowing for increased model capacity without a corresponding surge in computational demands.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 1,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 79,
                                    "end_line_num": 87
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1038,
                            "end_ix": 1050,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated | OpenReview",
                                "url": "https://openreview.net/forum?id=cit3SNnZ6Q#:~:text=Abstract%3A%20We%20introduce%2C%20Q,Sparse%20is%20effective%20in%20different",
                                "text": "Abstract: We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 17,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 26,
                                    "end_line_num": 34
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1221,
                            "end_ix": 1233,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2203.02155] Training language models to follow instructions with human feedback",
                                "url": "https://arxiv.org/abs/2203.02155#:~:text=and%20prompts%20submitted%20through%20the,Even%20though",
                                "text": "and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 27,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 61,
                                    "end_line_num": 70
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1295,
                            "end_ix": 1307,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2203.02155] Training language models to follow instructions with human feedback",
                                "url": "https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,Even%20though",
                                "text": "reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 27,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 65,
                                    "end_line_num": 70
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1409,
                            "end_ix": 1421,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback",
                                "url": "https://arxiv.org/abs/2212.08073#:~:text=supervised%20phase%20we%20sample%20from,judged",
                                "text": "supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 69,
                                    "end_line_num": 77
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1578,
                            "end_ix": 1590,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback",
                                "url": "https://arxiv.org/abs/2212.08073#:~:text=supervised%20phase%20we%20sample%20from,judged",
                                "text": "supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 69,
                                    "end_line_num": 77
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1724,
                            "end_ix": 1736,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Fine-Tuning LLMs for Multi-Turn Conversations: A Technical Deep Dive",
                                "url": "https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=%2A%20Multi,training",
                                "text": "* Multi-Turn Complexity: While base models handle single exchanges well, maintaining context and coherence across nuanced multistep conversations requires further specialized post-training.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 62,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1792,
                            "end_ix": 1804,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Fine-Tuning LLMs for Multi-Turn Conversations: A Technical Deep Dive",
                                "url": "https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=%2A%20Multi,training",
                                "text": "* Multi-Turn Complexity: While base models handle single exchanges well, maintaining context and coherence across nuanced multistep conversations requires further specialized post-training.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 62,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1804,
                            "end_ix": 1816,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "LLM Agents for Education: Advances and Applications",
                                "url": "https://arxiv.org/html/2503.11733v1#:~:text=The%20strong%20natural%20language%20understanding,retrieval%2C%20and%20adaptive%20content%20generation",
                                "text": "The strong natural language understanding of Large Language Models (LLMs) and the task automation capabilities of LLM agents make them valuable for addressing challenges in education (Weng, 2023 ; Wang et al.,  47). First, memory enables LLM agents to retain both long-term knowledge about students’ study habits and short-term context from real-time interactions, enhancing contextual understanding and ensuring personalized learning experiences across various educational tasks (Zhang et al., 2024a). Second, tool use allows LLM agents to access external resources, perform complex calculations, and retrieve real-time information, enabling them to automate intricate educational tasks such as grading, knowledge retrieval, and adaptive content generation,",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 52,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 88,
                                    "end_line_num": 97
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 1910,
                            "end_ix": 1922,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                                "url": "https://arxiv.org/html/2409.20089v2#:~:text=Large%20language%20models%20,fusal%20F%20eature%20A%20dversarial",
                                "text": "Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing LLM safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature (Arditi et al., 2024). We further show that the operation of refusal feature ablation (RFA) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Re fusal F eature A dversarial",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 37,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 49,
                                    "end_line_num": 57
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2051,
                            "end_ix": 2063,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                                "url": "https://arxiv.org/html/2409.20089v2#:~:text=model%20safety,to%20existing%20adversarial%20training%20methods",
                                "text": "model safety. Based on these findings, we propose Re fusal F eature A dversarial T raining (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 37,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 57,
                                    "end_line_num": 62
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2173,
                            "end_ix": 2185,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                                "url": "https://arxiv.org/html/2412.16504v1#:~:text=Fine,learning%2C%20and%20knowledge%20unlearning%2C%20discussing",
                                "text": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 39,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 40,
                                    "end_line_num": 49
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2264,
                            "end_ix": 2278,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Curating Public Datasets for LLM Pretraining",
                                "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=Another%20critical%20filter%20is%20for,isn%E2%80%99t%20perfect%2C%20but%20it%20substantially",
                                "text": "Another critical filter is for personal or sensitive information (PII). To responsibly use public data, many pipelines now detect and remove PII like phone numbers, email addresses, physical addresses, or personal names in contexts that look private. This is often done with regex patterns (for phone numbers, SSNs, etc.) and Named Entity Recognition to catch person names. Some teams incorporate dedicated PII detection libraries or models (e.g. Microsoft Presidio or custom classifiers) to scrub out any confidential data. For instance, in the Dolma corpus, the portion derived from software repositories (commits, issue discussions) was processed with PII removal to exclude things like author emails in commit logs. Automatic PII filtering isn’t perfect, but it substantially",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 23,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 133,
                                    "end_line_num": 142
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2278,
                            "end_ix": 2290,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                                "url": "https://arxiv.org/html/2412.16504v1#:~:text=Fine,learning%2C%20and%20knowledge%20unlearning%2C%20discussing",
                                "text": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 39,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 40,
                                    "end_line_num": 49
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2417,
                            "end_ix": 2429,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "LLM Agents for Education: Advances and Applications",
                                "url": "https://arxiv.org/html/2503.11733v1#:~:text=The%20strong%20natural%20language%20understanding,retrieval%2C%20and%20adaptive%20content%20generation",
                                "text": "The strong natural language understanding of Large Language Models (LLMs) and the task automation capabilities of LLM agents make them valuable for addressing challenges in education (Weng, 2023 ; Wang et al.,  47). First, memory enables LLM agents to retain both long-term knowledge about students’ study habits and short-term context from real-time interactions, enhancing contextual understanding and ensuring personalized learning experiences across various educational tasks (Zhang et al., 2024a). Second, tool use allows LLM agents to access external resources, perform complex calculations, and retrieve real-time information, enabling them to automate intricate educational tasks such as grading, knowledge retrieval, and adaptive content generation,",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 52,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 88,
                                    "end_line_num": 97
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2589,
                            "end_ix": 2603,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                                "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=We%20show%20that%20most%20studies,comprehensiveness",
                                "text": "We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness,",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 54,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 105,
                                    "end_line_num": 113
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2640,
                            "end_ix": 2654,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                                "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=Large%20language%20models%20,show%20potential%2C%20the%20review%20highlights",
                                "text": "Large language models (LLMs) are computer programs that can generate human-like text. They promise to improve patient education and expand access to medical information by helping patients better understand health conditions and treatment options. However, more information is needed about how these tools are used in patient care and the challenges they present. In this review, researchers analyzed 89 studies from 2022 to 2023 covering 29 medical specialties. These studies explored ways LLMs are used: for example, answering patient questions, summarizing or translating medical texts, and supporting clinical paperwork. While these tools show potential, the review highlights",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 54,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 124,
                                    "end_line_num": 132
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2710,
                            "end_ix": 2724,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                                "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=We%20show%20that%20most%20studies,comprehensiveness",
                                "text": "We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness,",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 54,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 105,
                                    "end_line_num": 113
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2724,
                            "end_ix": 2738,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                                "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=Large%20language%20models%20,show%20potential%2C%20the%20review%20highlights",
                                "text": "Large language models (LLMs) are computer programs that can generate human-like text. They promise to improve patient education and expand access to medical information by helping patients better understand health conditions and treatment options. However, more information is needed about how these tools are used in patient care and the challenges they present. In this review, researchers analyzed 89 studies from 2022 to 2023 covering 29 medical specialties. These studies explored ways LLMs are used: for example, answering patient questions, summarizing or translating medical texts, and supporting clinical paperwork. While these tools show potential, the review highlights",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 54,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 124,
                                    "end_line_num": 132
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2827,
                            "end_ix": 2840,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "A Survey on Large Language Models for Code Generation",
                                "url": "https://arxiv.org/html/2406.00515v1#:~:text=2023%29,US",
                                "text": "2023). This area has garnered substantial interest from both academia and industry, as evidenced by the development of tools like GitHub Copilot^{2}^{2}2https://github.com/features/copilot (Chen et al., 2021), CodeGeeX^{3}^{3}3https://codegeex.cn/en-US",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 50,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 99,
                                    "end_line_num": 102
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 2872,
                            "end_ix": 2884,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "A Survey on Large Language Models for Code Generation",
                                "url": "https://arxiv.org/html/2406.00515v1#:~:text=The%20advent%20of%20Large%20Language,Ren",
                                "text": "The advent of Large Language Models (LLMs) such as ChatGPT^{1}^{1}1https://chat.openai.com (OpenAI, 2022 ) has profoundly transformed the landscape of automated code-related tasks (Chen et al.,  36), including code completion (Wang and Li, 2021 ; Lu et al.,  38; Guo et al., 2023 ; Wu et al.,  40), code translation (Lachaux et al., 2020 ; Szafraniec et al.,  42; Chen et al., 2018 ), and code repair (Jiang et al.,  44; Olausson et al., 2023 ; Parasaram et al.,  46). A particularly intriguing application of LLMs is code generation, a task that involves producing source code from natural language descriptions. Despite varying definitions across studies (Ren",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 50,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 85,
                                    "end_line_num": 94
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3080,
                            "end_ix": 3092,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                                "url": "https://arxiv.org/abs/2201.11903#:~:text=,parameter",
                                "text": "> Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 43,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 60
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3178,
                            "end_ix": 3190,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey",
                                "url": "https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a",
                                "text": "> Abstract:Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non- transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 48,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 60
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3281,
                            "end_ix": 3291,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "GPT-4 | OpenAI",
                                "url": "https://openai.com/index/gpt-4-research/#:~:text=GPT%E2%80%914%20can%20accept%20a%20prompt,range%20of%20domains%E2%80%94including%20documents%20with",
                                "text": "GPT‑4 can accept a prompt of text and images, which—parallel to the text-only setting—lets the user specify any vision or language task. Specifically, it generates text outputs (natural language, code, etc.) given inputs consisting of interspersed text and images. Over a range of domains—including documents with",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 60,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1,
                                    "end_line_num": 4
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3491,
                            "end_ix": 3503,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools",
                                "url": "https://arxiv.org/abs/2302.04761#:~:text=,A",
                                "text": "> Abstract:Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 61
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3738,
                            "end_ix": 3750,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2305.12474] Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
                                "url": "https://arxiv.org/abs/2305.12474#:~:text=the%20performance%20of%20LLMs,exhibit%20significant%20performance%20disparities%20across",
                                "text": "the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and this http URL findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 63,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 61
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 3835,
                            "end_ix": 3847,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2211.09110] Holistic Evaluation of Language Models",
                                "url": "https://arxiv.org/abs/2211.09110#:~:text=,of%2016%20core%20scenarios%20when",
                                "text": "> Abstract:Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 65,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 62,
                                    "end_line_num": 71
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4000,
                            "end_ix": 4012,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2211.09110] Holistic Evaluation of Language Models",
                                "url": "https://arxiv.org/abs/2211.09110#:~:text=,of%2016%20core%20scenarios%20when",
                                "text": "> Abstract:Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 65,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 62,
                                    "end_line_num": 71
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4012,
                            "end_ix": 4026,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                                "url": "https://arxiv.org/abs/2206.04615#:~:text=benchmark%20%28BIG,tasks%20in%20order%20to%20provide",
                                "text": "benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 73,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 189,
                                    "end_line_num": 197
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4098,
                            "end_ix": 4112,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                                "url": "https://arxiv.org/abs/2206.04615#:~:text=benchmark%20%28BIG,tasks%20in%20order%20to%20provide",
                                "text": "benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 73,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 189,
                                    "end_line_num": 197
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4144,
                            "end_ix": 4158,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                                "url": "https://arxiv.org/abs/2206.04615#:~:text=commonly%20involve%20a%20large%20knowledge,can%20be%20improved%20with%20prompting",
                                "text": "commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 73,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 202,
                                    "end_line_num": 206
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4236,
                            "end_ix": 4248,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Detecting hallucinations in large language models using semantic entropy | Nature",
                                "url": "https://www.nature.com/articles/s41586-024-07421-0#:~:text=%E2%80%98hallucinate%E2%80%99%20false%20outputs%20and%20unsubstantiated,based%20uncertainty%20estimators",
                                "text": "‘hallucinate’ false outputs and unsubstantiated answers3 , 24. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5  or untrue facts in news articles 26 and even posing a risk to human life in medical domains such as radiology7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 28. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 75,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 69,
                                    "end_line_num": 77
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4277,
                            "end_ix": 4289,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2203.02155] Training language models to follow instructions with human feedback",
                                "url": "https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,Even%20though",
                                "text": "reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 27,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 65,
                                    "end_line_num": 70
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4305,
                            "end_ix": 4317,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Detecting hallucinations in large language models using semantic entropy | Nature",
                                "url": "https://www.nature.com/articles/s41586-024-07421-0#:~:text=%E2%80%98hallucinate%E2%80%99%20false%20outputs%20and%20unsubstantiated,based%20uncertainty%20estimators",
                                "text": "‘hallucinate’ false outputs and unsubstantiated answers3 , 24. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5  or untrue facts in news articles 26 and even posing a risk to human life in medical domains such as radiology7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 28. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 75,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 69,
                                    "end_line_num": 77
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4356,
                            "end_ix": 4368,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                                "url": "https://arxiv.org/html/2409.20089v2#:~:text=Large%20language%20models%20,fusal%20F%20eature%20A%20dversarial",
                                "text": "Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing LLM safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature (Arditi et al., 2024). We further show that the operation of refusal feature ablation (RFA) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Re fusal F eature A dversarial",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 37,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 49,
                                    "end_line_num": 57
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4368,
                            "end_ix": 4380,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                                "url": "https://arxiv.org/html/2409.20089v2#:~:text=model%20safety,to%20existing%20adversarial%20training%20methods",
                                "text": "model safety. Based on these findings, we propose Re fusal F eature A dversarial T raining (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 37,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 57,
                                    "end_line_num": 62
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4444,
                            "end_ix": 4458,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                                "url": "https://arxiv.org/abs/2206.04615#:~:text=commonly%20involve%20a%20large%20knowledge,can%20be%20improved%20with%20prompting",
                                "text": "commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 73,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 202,
                                    "end_line_num": 206
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4555,
                            "end_ix": 4566,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "",
                                "url": "https://arxiv.org/pdf/2407.06204#:~:text=is%20based%20on%20a%20simple,corresponding%20surge%20in%20computational%20demands",
                                "text": "is based on a simple yet powerful idea: different parts of a model, known as experts, specialize in different tasks or aspects of the data. With this paradigm, only pertinent experts are engaged for a given input, keeping the com\u0002putational cost in check while still benefiting from a large pool of specialized knowledge. This scalable and flexible innovation has offered an effective approach for adhering to the scaling law, allowing for increased model capacity without a corresponding surge in computational demands.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 1,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 79,
                                    "end_line_num": 87
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4566,
                            "end_ix": 4578,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
                                "url": "https://arxiv.org/html/2403.07816v1#:~:text=We%20investigate%20efficient%20methods%20for,Merge%20method%2C%20which",
                                "text": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 15,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 60
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4578,
                            "end_ix": 4590,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated | OpenReview",
                                "url": "https://openreview.net/forum?id=cit3SNnZ6Q#:~:text=Abstract%3A%20We%20introduce%2C%20Q,Sparse%20is%20effective%20in%20different",
                                "text": "Abstract: We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 17,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 26,
                                    "end_line_num": 34
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4590,
                            "end_ix": 4602,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                                "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=,for%20further%20filtering%20and%20weighting",
                                "text": "# Today, we’re releasing a new version of the RedPajama dataset, with 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations that can be used for further filtering and weighting.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 20,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 50,
                                    "end_line_num": 53
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4602,
                            "end_ix": 4614,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                                "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=A%20central%20ingredient%20to%20state,sources%20are%20the%20crawls%20made",
                                "text": "A central ingredient to state-of-the-art open LLMs like Llama, Mistral, Falcon, MPT, and the RedPajama models is the large amounts of high-quality data that these models are trained on. For example, Llama 2 is trained on 2.4 trillion carefully curated tokens. The most prominent data sources are the crawls made",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 22,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 79,
                                    "end_line_num": 82
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4614,
                            "end_ix": 4626,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Curating Public Datasets for LLM Pretraining",
                                "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=content%29,v2%3A%20An%20open%20dataset",
                                "text": "content). Large open datasets such as RedPajama-Data-v2 (released late 2023) demonstrate these collection methods: it gathered over 100 billion web documents from 84 CommonCrawl snapshots, then filtered and deduplicated them to 30 billion documents (~30 trillion tokens) ( 6 RedPajama-Data-v2: An open dataset",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 23,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 90,
                                    "end_line_num": 93
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4626,
                            "end_ix": 4640,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
                                "url": "https://arxiv.org/html/2406.14491v1#:~:text=Except%20for%20the%20pre,training%20scenarios",
                                "text": "Except for the pre-training data, Instruction Pre-Training keeps all other pre- training settings the same as Vanilla Pre-Training: training with the next-token prediction objective (Radford et al., 2018) and computing loss on all tokens. We conduct both general pre-training from scratch and domain-adaptive continued pre-training to verify the effectiveness in different pre-training scenarios.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 25,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 251,
                                    "end_line_num": 259
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4640,
                            "end_ix": 4652,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2203.02155] Training language models to follow instructions with human feedback",
                                "url": "https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,Even%20though",
                                "text": "reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 27,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 65,
                                    "end_line_num": 70
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4652,
                            "end_ix": 4664,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback",
                                "url": "https://arxiv.org/abs/2212.08073#:~:text=supervised%20phase%20we%20sample%20from,judged",
                                "text": "supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 32,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 69,
                                    "end_line_num": 77
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4664,
                            "end_ix": 4676,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Robust LLM safeguarding via refusal feature adversarial training",
                                "url": "https://arxiv.org/html/2409.20089v2#:~:text=model%20safety,to%20existing%20adversarial%20training%20methods",
                                "text": "model safety. Based on these findings, we propose Re fusal F eature A dversarial T raining (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 37,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 57,
                                    "end_line_num": 62
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4676,
                            "end_ix": 4688,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                                "url": "https://arxiv.org/html/2412.16504v1#:~:text=Fine,learning%2C%20and%20knowledge%20unlearning%2C%20discussing",
                                "text": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 39,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 40,
                                    "end_line_num": 49
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4688,
                            "end_ix": 4700,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Fine-Tuning LLMs for Multi-Turn Conversations: A Technical Deep Dive",
                                "url": "https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=%2A%20Multi,training",
                                "text": "* Multi-Turn Complexity: While base models handle single exchanges well, maintaining context and coherence across nuanced multistep conversations requires further specialized post-training.",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 41,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 62,
                                    "end_line_num": 64
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4700,
                            "end_ix": 4712,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "LLM Agents for Education: Advances and Applications",
                                "url": "https://arxiv.org/html/2503.11733v1#:~:text=The%20strong%20natural%20language%20understanding,retrieval%2C%20and%20adaptive%20content%20generation",
                                "text": "The strong natural language understanding of Large Language Models (LLMs) and the task automation capabilities of LLM agents make them valuable for addressing challenges in education (Weng, 2023 ; Wang et al.,  47). First, memory enables LLM agents to retain both long-term knowledge about students’ study habits and short-term context from real-time interactions, enhancing contextual understanding and ensuring personalized learning experiences across various educational tasks (Zhang et al., 2024a). Second, tool use allows LLM agents to access external resources, perform complex calculations, and retrieve real-time information, enabling them to automate intricate educational tasks such as grading, knowledge retrieval, and adaptive content generation,",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 52,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 88,
                                    "end_line_num": 97
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4712,
                            "end_ix": 4726,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                                "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=We%20show%20that%20most%20studies,comprehensiveness",
                                "text": "We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness,",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 54,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 105,
                                    "end_line_num": 113
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4726,
                            "end_ix": 4738,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "A Survey on Large Language Models for Code Generation",
                                "url": "https://arxiv.org/html/2406.00515v1#:~:text=The%20advent%20of%20Large%20Language,Ren",
                                "text": "The advent of Large Language Models (LLMs) such as ChatGPT^{1}^{1}1https://chat.openai.com (OpenAI, 2022 ) has profoundly transformed the landscape of automated code-related tasks (Chen et al.,  36), including code completion (Wang and Li, 2021 ; Lu et al.,  38; Guo et al., 2023 ; Wu et al.,  40), code translation (Lachaux et al., 2020 ; Szafraniec et al.,  42; Chen et al., 2018 ), and code repair (Jiang et al.,  44; Olausson et al., 2023 ; Parasaram et al.,  46). A particularly intriguing application of LLMs is code generation, a task that involves producing source code from natural language descriptions. Despite varying definitions across studies (Ren",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 50,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 85,
                                    "end_line_num": 94
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4738,
                            "end_ix": 4751,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "A Survey on Large Language Models for Code Generation",
                                "url": "https://arxiv.org/html/2406.00515v1#:~:text=2023%29,US",
                                "text": "2023). This area has garnered substantial interest from both academia and industry, as evidenced by the development of tools like GitHub Copilot^{2}^{2}2https://github.com/features/copilot (Chen et al., 2021), CodeGeeX^{3}^{3}3https://codegeex.cn/en-US",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 50,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 99,
                                    "end_line_num": 102
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4751,
                            "end_ix": 4763,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                                "url": "https://arxiv.org/abs/2201.11903#:~:text=,parameter",
                                "text": "> Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 43,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 60
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4763,
                            "end_ix": 4775,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey",
                                "url": "https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a",
                                "text": "> Abstract:Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non- transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 48,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 60
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4775,
                            "end_ix": 4787,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools",
                                "url": "https://arxiv.org/abs/2302.04761#:~:text=,A",
                                "text": "> Abstract:Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 45,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 52,
                                    "end_line_num": 61
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4787,
                            "end_ix": 4797,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "GPT-4 | OpenAI",
                                "url": "https://openai.com/index/gpt-4-research/#:~:text=GPT%E2%80%914%20can%20accept%20a%20prompt,range%20of%20domains%E2%80%94including%20documents%20with",
                                "text": "GPT‑4 can accept a prompt of text and images, which—parallel to the text-only setting—lets the user specify any vision or language task. Specifically, it generates text outputs (natural language, code, etc.) given inputs consisting of interspersed text and images. Over a range of domains—including documents with",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 60,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 1,
                                    "end_line_num": 4
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4797,
                            "end_ix": 4809,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2211.09110] Holistic Evaluation of Language Models",
                                "url": "https://arxiv.org/abs/2211.09110#:~:text=,of%2016%20core%20scenarios%20when",
                                "text": "> Abstract:Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 65,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 62,
                                    "end_line_num": 71
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4809,
                            "end_ix": 4821,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2305.12474] Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
                                "url": "https://arxiv.org/abs/2305.12474#:~:text=the%20performance%20of%20LLMs,exhibit%20significant%20performance%20disparities%20across",
                                "text": "the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and this http URL findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 63,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 58,
                                    "end_line_num": 61
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4821,
                            "end_ix": 4835,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                                "url": "https://arxiv.org/abs/2206.04615#:~:text=benchmark%20%28BIG,tasks%20in%20order%20to%20provide",
                                "text": "benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 73,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 189,
                                    "end_line_num": 197
                                },
                                "og_tags": null
                            }
                        },
                        {
                            "start_ix": 4835,
                            "end_ix": 4847,
                            "citation_format_type": "tether_v4",
                            "metadata": {
                                "type": "webpage",
                                "title": "Detecting hallucinations in large language models using semantic entropy | Nature",
                                "url": "https://www.nature.com/articles/s41586-024-07421-0#:~:text=%E2%80%98hallucinate%E2%80%99%20false%20outputs%20and%20unsubstantiated,based%20uncertainty%20estimators",
                                "text": "‘hallucinate’ false outputs and unsubstantiated answers3 , 24. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5  or untrue facts in news articles 26 and even posing a risk to human life in medical domains such as radiology7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 28. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators",
                                "pub_date": null,
                                "extra": {
                                    "cited_message_idx": 75,
                                    "search_result_idx": null,
                                    "evidence_text": "",
                                    "start_line_num": 69,
                                    "end_line_num": 77
                                },
                                "og_tags": null
                            }
                        }
                    ],
                    "content_references": [
                        {
                            "matched_text": "【1†L79-L87】",
                            "start_idx": 157,
                            "end_idx": 168,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "",
                            "url": "https://arxiv.org/pdf/2407.06204#:~:text=is%20based%20on%20a%20simple,corresponding%20surge%20in%20computational%20demands",
                            "pub_date": null,
                            "snippet": "is based on a simple yet powerful idea: different parts of a model, known as experts, specialize in different tasks or aspects of the data. With this paradigm, only pertinent experts are engaged for a given input, keeping the com\u0002putational cost in check while still benefiting from a large pool of specialized knowledge. This scalable and flexible innovation has offered an effective approach for adhering to the scaling law, allowing for increased model capacity without a corresponding surge in computational demands.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【15†L52-L60】",
                            "start_idx": 257,
                            "end_idx": 269,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
                            "url": "https://arxiv.org/html/2403.07816v1#:~:text=We%20investigate%20efficient%20methods%20for,Merge%20method%2C%20which",
                            "pub_date": null,
                            "snippet": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【17†L26-L34】",
                            "start_idx": 334,
                            "end_idx": 346,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated | OpenReview",
                            "url": "https://openreview.net/forum?id=cit3SNnZ6Q#:~:text=Abstract%3A%20We%20introduce%2C%20Q,Sparse%20is%20effective%20in%20different",
                            "pub_date": null,
                            "snippet": "Abstract: We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different",
                            "attribution": "openreview.net",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【25†L251-L259】",
                            "start_idx": 481,
                            "end_idx": 495,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
                            "url": "https://arxiv.org/html/2406.14491v1#:~:text=Except%20for%20the%20pre,training%20scenarios",
                            "pub_date": null,
                            "snippet": "Except for the pre-training data, Instruction Pre-Training keeps all other pre- training settings the same as Vanilla Pre-Training: training with the next-token prediction objective (Radford et al., 2018) and computing loss on all tokens. We conduct both general pre-training from scratch and domain-adaptive continued pre-training to verify the effectiveness in different pre-training scenarios.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【20†L50-L53】",
                            "start_idx": 692,
                            "end_idx": 704,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                            "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=,for%20further%20filtering%20and%20weighting",
                            "pub_date": null,
                            "snippet": "# Today, we’re releasing a new version of the RedPajama dataset, with 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations that can be used for further filtering and weighting.",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【23†L90-L93】",
                            "start_idx": 704,
                            "end_idx": 716,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Curating Public Datasets for LLM Pretraining",
                            "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=content%29,v2%3A%20An%20open%20dataset",
                            "pub_date": null,
                            "snippet": "content). Large open datasets such as RedPajama-Data-v2 (released late 2023) demonstrate these collection methods: it gathered over 100 billion web documents from 84 CommonCrawl snapshots, then filtered and deduplicated them to 30 billion documents (~30 trillion tokens) ( 6 RedPajama-Data-v2: An open dataset",
                            "attribution": "rohan-paul.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【22†L79-L82】",
                            "start_idx": 751,
                            "end_idx": 763,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                            "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=A%20central%20ingredient%20to%20state,sources%20are%20the%20crawls%20made",
                            "pub_date": null,
                            "snippet": "A central ingredient to state-of-the-art open LLMs like Llama, Mistral, Falcon, MPT, and the RedPajama models is the large amounts of high-quality data that these models are trained on. For example, Llama 2 is trained on 2.4 trillion carefully curated tokens. The most prominent data sources are the crawls made",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【23†L90-L93】",
                            "start_idx": 763,
                            "end_idx": 775,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Curating Public Datasets for LLM Pretraining",
                            "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=content%29,v2%3A%20An%20open%20dataset",
                            "pub_date": null,
                            "snippet": "content). Large open datasets such as RedPajama-Data-v2 (released late 2023) demonstrate these collection methods: it gathered over 100 billion web documents from 84 CommonCrawl snapshots, then filtered and deduplicated them to 30 billion documents (~30 trillion tokens) ( 6 RedPajama-Data-v2: An open dataset",
                            "attribution": "rohan-paul.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【23†L133-L142】",
                            "start_idx": 868,
                            "end_idx": 882,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Curating Public Datasets for LLM Pretraining",
                            "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=Another%20critical%20filter%20is%20for,isn%E2%80%99t%20perfect%2C%20but%20it%20substantially",
                            "pub_date": null,
                            "snippet": "Another critical filter is for personal or sensitive information (PII). To responsibly use public data, many pipelines now detect and remove PII like phone numbers, email addresses, physical addresses, or personal names in contexts that look private. This is often done with regex patterns (for phone numbers, SSNs, etc.) and Named Entity Recognition to catch person names. Some teams incorporate dedicated PII detection libraries or models (e.g. Microsoft Presidio or custom classifiers) to scrub out any confidential data. For instance, in the Dolma corpus, the portion derived from software repositories (commits, issue discussions) was processed with PII removal to exclude things like author emails in commit logs. Automatic PII filtering isn’t perfect, but it substantially",
                            "attribution": "rohan-paul.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【15†L75-L82】",
                            "start_idx": 950,
                            "end_idx": 962,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
                            "url": "https://arxiv.org/html/2403.07816v1#:~:text=multilinguality%20%28Zhao%20et%C2%A0al,41%3B%20Gemini%20Team%2C%202023",
                            "pub_date": null,
                            "snippet": "multilinguality (Zhao et al., 2024 ), etc. Training such LLMs requires a large amount of compute and data, exceeding thousands of GPUs and trillions of tokens. The training parallelization is typically done by maintaining multiple copies of the model on different GPUs and keeping them synchronized after each weight update. The cost of this frequent communication is the main bottleneck in scaling the training to more GPUs. Besides this issue, synchronized training is more vulnerable to hardware failures as a single failed GPU can cause the whole training to halt (Zhang et al.,  41; Gemini Team, 2023).",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【1†L79-L87】",
                            "start_idx": 1027,
                            "end_idx": 1038,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "",
                            "url": "https://arxiv.org/pdf/2407.06204#:~:text=is%20based%20on%20a%20simple,corresponding%20surge%20in%20computational%20demands",
                            "pub_date": null,
                            "snippet": "is based on a simple yet powerful idea: different parts of a model, known as experts, specialize in different tasks or aspects of the data. With this paradigm, only pertinent experts are engaged for a given input, keeping the com\u0002putational cost in check while still benefiting from a large pool of specialized knowledge. This scalable and flexible innovation has offered an effective approach for adhering to the scaling law, allowing for increased model capacity without a corresponding surge in computational demands.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【17†L26-L34】",
                            "start_idx": 1038,
                            "end_idx": 1050,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated | OpenReview",
                            "url": "https://openreview.net/forum?id=cit3SNnZ6Q#:~:text=Abstract%3A%20We%20introduce%2C%20Q,Sparse%20is%20effective%20in%20different",
                            "pub_date": null,
                            "snippet": "Abstract: We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different",
                            "attribution": "openreview.net",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【27†L61-L70】",
                            "start_idx": 1221,
                            "end_idx": 1233,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2203.02155] Training language models to follow instructions with human feedback",
                            "url": "https://arxiv.org/abs/2203.02155#:~:text=and%20prompts%20submitted%20through%20the,Even%20though",
                            "pub_date": null,
                            "snippet": "and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【27†L65-L70】",
                            "start_idx": 1295,
                            "end_idx": 1307,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2203.02155] Training language models to follow instructions with human feedback",
                            "url": "https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,Even%20though",
                            "pub_date": null,
                            "snippet": "reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L69-L77】",
                            "start_idx": 1409,
                            "end_idx": 1421,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback",
                            "url": "https://arxiv.org/abs/2212.08073#:~:text=supervised%20phase%20we%20sample%20from,judged",
                            "pub_date": null,
                            "snippet": "supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L69-L77】",
                            "start_idx": 1578,
                            "end_idx": 1590,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback",
                            "url": "https://arxiv.org/abs/2212.08073#:~:text=supervised%20phase%20we%20sample%20from,judged",
                            "pub_date": null,
                            "snippet": "supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L62-L64】",
                            "start_idx": 1724,
                            "end_idx": 1736,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Fine-Tuning LLMs for Multi-Turn Conversations: A Technical Deep Dive",
                            "url": "https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=%2A%20Multi,training",
                            "pub_date": null,
                            "snippet": "* Multi-Turn Complexity: While base models handle single exchanges well, maintaining context and coherence across nuanced multistep conversations requires further specialized post-training.",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L62-L64】",
                            "start_idx": 1792,
                            "end_idx": 1804,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Fine-Tuning LLMs for Multi-Turn Conversations: A Technical Deep Dive",
                            "url": "https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=%2A%20Multi,training",
                            "pub_date": null,
                            "snippet": "* Multi-Turn Complexity: While base models handle single exchanges well, maintaining context and coherence across nuanced multistep conversations requires further specialized post-training.",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【52†L88-L97】",
                            "start_idx": 1804,
                            "end_idx": 1816,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "LLM Agents for Education: Advances and Applications",
                            "url": "https://arxiv.org/html/2503.11733v1#:~:text=The%20strong%20natural%20language%20understanding,retrieval%2C%20and%20adaptive%20content%20generation",
                            "pub_date": null,
                            "snippet": "The strong natural language understanding of Large Language Models (LLMs) and the task automation capabilities of LLM agents make them valuable for addressing challenges in education (Weng, 2023 ; Wang et al.,  47). First, memory enables LLM agents to retain both long-term knowledge about students’ study habits and short-term context from real-time interactions, enhancing contextual understanding and ensuring personalized learning experiences across various educational tasks (Zhang et al., 2024a). Second, tool use allows LLM agents to access external resources, perform complex calculations, and retrieve real-time information, enabling them to automate intricate educational tasks such as grading, knowledge retrieval, and adaptive content generation,",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【37†L49-L57】",
                            "start_idx": 1910,
                            "end_idx": 1922,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Robust LLM safeguarding via refusal feature adversarial training",
                            "url": "https://arxiv.org/html/2409.20089v2#:~:text=Large%20language%20models%20,fusal%20F%20eature%20A%20dversarial",
                            "pub_date": null,
                            "snippet": "Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing LLM safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature (Arditi et al., 2024). We further show that the operation of refusal feature ablation (RFA) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Re fusal F eature A dversarial",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【37†L57-L62】",
                            "start_idx": 2051,
                            "end_idx": 2063,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Robust LLM safeguarding via refusal feature adversarial training",
                            "url": "https://arxiv.org/html/2409.20089v2#:~:text=model%20safety,to%20existing%20adversarial%20training%20methods",
                            "pub_date": null,
                            "snippet": "model safety. Based on these findings, we propose Re fusal F eature A dversarial T raining (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【39†L40-L49】",
                            "start_idx": 2173,
                            "end_idx": 2185,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                            "url": "https://arxiv.org/html/2412.16504v1#:~:text=Fine,learning%2C%20and%20knowledge%20unlearning%2C%20discussing",
                            "pub_date": null,
                            "snippet": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【23†L133-L142】",
                            "start_idx": 2264,
                            "end_idx": 2278,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Curating Public Datasets for LLM Pretraining",
                            "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=Another%20critical%20filter%20is%20for,isn%E2%80%99t%20perfect%2C%20but%20it%20substantially",
                            "pub_date": null,
                            "snippet": "Another critical filter is for personal or sensitive information (PII). To responsibly use public data, many pipelines now detect and remove PII like phone numbers, email addresses, physical addresses, or personal names in contexts that look private. This is often done with regex patterns (for phone numbers, SSNs, etc.) and Named Entity Recognition to catch person names. Some teams incorporate dedicated PII detection libraries or models (e.g. Microsoft Presidio or custom classifiers) to scrub out any confidential data. For instance, in the Dolma corpus, the portion derived from software repositories (commits, issue discussions) was processed with PII removal to exclude things like author emails in commit logs. Automatic PII filtering isn’t perfect, but it substantially",
                            "attribution": "rohan-paul.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【39†L40-L49】",
                            "start_idx": 2278,
                            "end_idx": 2290,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                            "url": "https://arxiv.org/html/2412.16504v1#:~:text=Fine,learning%2C%20and%20knowledge%20unlearning%2C%20discussing",
                            "pub_date": null,
                            "snippet": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【52†L88-L97】",
                            "start_idx": 2417,
                            "end_idx": 2429,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "LLM Agents for Education: Advances and Applications",
                            "url": "https://arxiv.org/html/2503.11733v1#:~:text=The%20strong%20natural%20language%20understanding,retrieval%2C%20and%20adaptive%20content%20generation",
                            "pub_date": null,
                            "snippet": "The strong natural language understanding of Large Language Models (LLMs) and the task automation capabilities of LLM agents make them valuable for addressing challenges in education (Weng, 2023 ; Wang et al.,  47). First, memory enables LLM agents to retain both long-term knowledge about students’ study habits and short-term context from real-time interactions, enhancing contextual understanding and ensuring personalized learning experiences across various educational tasks (Zhang et al., 2024a). Second, tool use allows LLM agents to access external resources, perform complex calculations, and retrieve real-time information, enabling them to automate intricate educational tasks such as grading, knowledge retrieval, and adaptive content generation,",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【54†L105-L113】",
                            "start_idx": 2589,
                            "end_idx": 2603,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                            "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=We%20show%20that%20most%20studies,comprehensiveness",
                            "pub_date": null,
                            "snippet": "We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness,",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【54†L124-L132】",
                            "start_idx": 2640,
                            "end_idx": 2654,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                            "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=Large%20language%20models%20,show%20potential%2C%20the%20review%20highlights",
                            "pub_date": null,
                            "snippet": "Large language models (LLMs) are computer programs that can generate human-like text. They promise to improve patient education and expand access to medical information by helping patients better understand health conditions and treatment options. However, more information is needed about how these tools are used in patient care and the challenges they present. In this review, researchers analyzed 89 studies from 2022 to 2023 covering 29 medical specialties. These studies explored ways LLMs are used: for example, answering patient questions, summarizing or translating medical texts, and supporting clinical paperwork. While these tools show potential, the review highlights",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【54†L105-L113】",
                            "start_idx": 2710,
                            "end_idx": 2724,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                            "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=We%20show%20that%20most%20studies,comprehensiveness",
                            "pub_date": null,
                            "snippet": "We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness,",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【54†L124-L132】",
                            "start_idx": 2724,
                            "end_idx": 2738,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                            "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=Large%20language%20models%20,show%20potential%2C%20the%20review%20highlights",
                            "pub_date": null,
                            "snippet": "Large language models (LLMs) are computer programs that can generate human-like text. They promise to improve patient education and expand access to medical information by helping patients better understand health conditions and treatment options. However, more information is needed about how these tools are used in patient care and the challenges they present. In this review, researchers analyzed 89 studies from 2022 to 2023 covering 29 medical specialties. These studies explored ways LLMs are used: for example, answering patient questions, summarizing or translating medical texts, and supporting clinical paperwork. While these tools show potential, the review highlights",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【50†L99-L102】",
                            "start_idx": 2827,
                            "end_idx": 2840,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "A Survey on Large Language Models for Code Generation",
                            "url": "https://arxiv.org/html/2406.00515v1#:~:text=2023%29,US",
                            "pub_date": null,
                            "snippet": "2023). This area has garnered substantial interest from both academia and industry, as evidenced by the development of tools like GitHub Copilot^{2}^{2}2https://github.com/features/copilot (Chen et al., 2021), CodeGeeX^{3}^{3}3https://codegeex.cn/en-US",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【50†L85-L94】",
                            "start_idx": 2872,
                            "end_idx": 2884,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "A Survey on Large Language Models for Code Generation",
                            "url": "https://arxiv.org/html/2406.00515v1#:~:text=The%20advent%20of%20Large%20Language,Ren",
                            "pub_date": null,
                            "snippet": "The advent of Large Language Models (LLMs) such as ChatGPT^{1}^{1}1https://chat.openai.com (OpenAI, 2022 ) has profoundly transformed the landscape of automated code-related tasks (Chen et al.,  36), including code completion (Wang and Li, 2021 ; Lu et al.,  38; Guo et al., 2023 ; Wu et al.,  40), code translation (Lachaux et al., 2020 ; Szafraniec et al.,  42; Chen et al., 2018 ), and code repair (Jiang et al.,  44; Olausson et al., 2023 ; Parasaram et al.,  46). A particularly intriguing application of LLMs is code generation, a task that involves producing source code from natural language descriptions. Despite varying definitions across studies (Ren",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【43†L52-L60】",
                            "start_idx": 3080,
                            "end_idx": 3092,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                            "url": "https://arxiv.org/abs/2201.11903#:~:text=,parameter",
                            "pub_date": null,
                            "snippet": "> Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【48†L52-L60】",
                            "start_idx": 3178,
                            "end_idx": 3190,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey",
                            "url": "https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a",
                            "pub_date": null,
                            "snippet": "> Abstract:Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non- transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【60†L1-L4】",
                            "start_idx": 3281,
                            "end_idx": 3291,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "GPT-4 | OpenAI",
                            "url": "https://openai.com/index/gpt-4-research/#:~:text=GPT%E2%80%914%20can%20accept%20a%20prompt,range%20of%20domains%E2%80%94including%20documents%20with",
                            "pub_date": null,
                            "snippet": "GPT‑4 can accept a prompt of text and images, which—parallel to the text-only setting—lets the user specify any vision or language task. Specifically, it generates text outputs (natural language, code, etc.) given inputs consisting of interspersed text and images. Over a range of domains—including documents with",
                            "attribution": "openai.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【45†L52-L61】",
                            "start_idx": 3491,
                            "end_idx": 3503,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools",
                            "url": "https://arxiv.org/abs/2302.04761#:~:text=,A",
                            "pub_date": null,
                            "snippet": "> Abstract:Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【63†L58-L61】",
                            "start_idx": 3738,
                            "end_idx": 3750,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2305.12474] Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
                            "url": "https://arxiv.org/abs/2305.12474#:~:text=the%20performance%20of%20LLMs,exhibit%20significant%20performance%20disparities%20across",
                            "pub_date": null,
                            "snippet": "the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and this http URL findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【65†L62-L71】",
                            "start_idx": 3835,
                            "end_idx": 3847,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2211.09110] Holistic Evaluation of Language Models",
                            "url": "https://arxiv.org/abs/2211.09110#:~:text=,of%2016%20core%20scenarios%20when",
                            "pub_date": null,
                            "snippet": "> Abstract:Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【65†L62-L71】",
                            "start_idx": 4000,
                            "end_idx": 4012,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2211.09110] Holistic Evaluation of Language Models",
                            "url": "https://arxiv.org/abs/2211.09110#:~:text=,of%2016%20core%20scenarios%20when",
                            "pub_date": null,
                            "snippet": "> Abstract:Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【73†L189-L197】",
                            "start_idx": 4012,
                            "end_idx": 4026,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                            "url": "https://arxiv.org/abs/2206.04615#:~:text=benchmark%20%28BIG,tasks%20in%20order%20to%20provide",
                            "pub_date": null,
                            "snippet": "benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【73†L189-L197】",
                            "start_idx": 4098,
                            "end_idx": 4112,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                            "url": "https://arxiv.org/abs/2206.04615#:~:text=benchmark%20%28BIG,tasks%20in%20order%20to%20provide",
                            "pub_date": null,
                            "snippet": "benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【73†L202-L206】",
                            "start_idx": 4144,
                            "end_idx": 4158,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                            "url": "https://arxiv.org/abs/2206.04615#:~:text=commonly%20involve%20a%20large%20knowledge,can%20be%20improved%20with%20prompting",
                            "pub_date": null,
                            "snippet": "commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【75†L69-L77】",
                            "start_idx": 4236,
                            "end_idx": 4248,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Detecting hallucinations in large language models using semantic entropy | Nature",
                            "url": "https://www.nature.com/articles/s41586-024-07421-0#:~:text=%E2%80%98hallucinate%E2%80%99%20false%20outputs%20and%20unsubstantiated,based%20uncertainty%20estimators",
                            "pub_date": null,
                            "snippet": "‘hallucinate’ false outputs and unsubstantiated answers3 , 24. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5  or untrue facts in news articles 26 and even posing a risk to human life in medical domains such as radiology7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 28. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【27†L65-L70】",
                            "start_idx": 4277,
                            "end_idx": 4289,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2203.02155] Training language models to follow instructions with human feedback",
                            "url": "https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,Even%20though",
                            "pub_date": null,
                            "snippet": "reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【75†L69-L77】",
                            "start_idx": 4305,
                            "end_idx": 4317,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Detecting hallucinations in large language models using semantic entropy | Nature",
                            "url": "https://www.nature.com/articles/s41586-024-07421-0#:~:text=%E2%80%98hallucinate%E2%80%99%20false%20outputs%20and%20unsubstantiated,based%20uncertainty%20estimators",
                            "pub_date": null,
                            "snippet": "‘hallucinate’ false outputs and unsubstantiated answers3 , 24. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5  or untrue facts in news articles 26 and even posing a risk to human life in medical domains such as radiology7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 28. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【37†L49-L57】",
                            "start_idx": 4356,
                            "end_idx": 4368,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Robust LLM safeguarding via refusal feature adversarial training",
                            "url": "https://arxiv.org/html/2409.20089v2#:~:text=Large%20language%20models%20,fusal%20F%20eature%20A%20dversarial",
                            "pub_date": null,
                            "snippet": "Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing LLM safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature (Arditi et al., 2024). We further show that the operation of refusal feature ablation (RFA) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Re fusal F eature A dversarial",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【37†L57-L62】",
                            "start_idx": 4368,
                            "end_idx": 4380,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Robust LLM safeguarding via refusal feature adversarial training",
                            "url": "https://arxiv.org/html/2409.20089v2#:~:text=model%20safety,to%20existing%20adversarial%20training%20methods",
                            "pub_date": null,
                            "snippet": "model safety. Based on these findings, we propose Re fusal F eature A dversarial T raining (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【73†L202-L206】",
                            "start_idx": 4444,
                            "end_idx": 4458,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                            "url": "https://arxiv.org/abs/2206.04615#:~:text=commonly%20involve%20a%20large%20knowledge,can%20be%20improved%20with%20prompting",
                            "pub_date": null,
                            "snippet": "commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【1†L79-L87】",
                            "start_idx": 4555,
                            "end_idx": 4566,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "",
                            "url": "https://arxiv.org/pdf/2407.06204#:~:text=is%20based%20on%20a%20simple,corresponding%20surge%20in%20computational%20demands",
                            "pub_date": null,
                            "snippet": "is based on a simple yet powerful idea: different parts of a model, known as experts, specialize in different tasks or aspects of the data. With this paradigm, only pertinent experts are engaged for a given input, keeping the com\u0002putational cost in check while still benefiting from a large pool of specialized knowledge. This scalable and flexible innovation has offered an effective approach for adhering to the scaling law, allowing for increased model capacity without a corresponding surge in computational demands.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【15†L52-L60】",
                            "start_idx": 4566,
                            "end_idx": 4578,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
                            "url": "https://arxiv.org/html/2403.07816v1#:~:text=We%20investigate%20efficient%20methods%20for,Merge%20method%2C%20which",
                            "pub_date": null,
                            "snippet": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【17†L26-L34】",
                            "start_idx": 4578,
                            "end_idx": 4590,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated | OpenReview",
                            "url": "https://openreview.net/forum?id=cit3SNnZ6Q#:~:text=Abstract%3A%20We%20introduce%2C%20Q,Sparse%20is%20effective%20in%20different",
                            "pub_date": null,
                            "snippet": "Abstract: We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different",
                            "attribution": "openreview.net",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【20†L50-L53】",
                            "start_idx": 4590,
                            "end_idx": 4602,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                            "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=,for%20further%20filtering%20and%20weighting",
                            "pub_date": null,
                            "snippet": "# Today, we’re releasing a new version of the RedPajama dataset, with 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations that can be used for further filtering and weighting.",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【22†L79-L82】",
                            "start_idx": 4602,
                            "end_idx": 4614,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "RedPajama-Data-v2: An open dataset with 30 trillion tokens for training large language models",
                            "url": "https://together.ai/blog/redpajama-data-v2#:~:text=,for%20further%20filtering%20and%20weighting#:~:text=A%20central%20ingredient%20to%20state,sources%20are%20the%20crawls%20made",
                            "pub_date": null,
                            "snippet": "A central ingredient to state-of-the-art open LLMs like Llama, Mistral, Falcon, MPT, and the RedPajama models is the large amounts of high-quality data that these models are trained on. For example, Llama 2 is trained on 2.4 trillion carefully curated tokens. The most prominent data sources are the crawls made",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【23†L90-L93】",
                            "start_idx": 4614,
                            "end_idx": 4626,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Curating Public Datasets for LLM Pretraining",
                            "url": "https://www.rohan-paul.com/p/curating-public-datasets-for-llm?source=queue#:~:text=content%29,v2%3A%20An%20open%20dataset",
                            "pub_date": null,
                            "snippet": "content). Large open datasets such as RedPajama-Data-v2 (released late 2023) demonstrate these collection methods: it gathered over 100 billion web documents from 84 CommonCrawl snapshots, then filtered and deduplicated them to 30 billion documents (~30 trillion tokens) ( 6 RedPajama-Data-v2: An open dataset",
                            "attribution": "rohan-paul.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【25†L251-L259】",
                            "start_idx": 4626,
                            "end_idx": 4640,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
                            "url": "https://arxiv.org/html/2406.14491v1#:~:text=Except%20for%20the%20pre,training%20scenarios",
                            "pub_date": null,
                            "snippet": "Except for the pre-training data, Instruction Pre-Training keeps all other pre- training settings the same as Vanilla Pre-Training: training with the next-token prediction objective (Radford et al., 2018) and computing loss on all tokens. We conduct both general pre-training from scratch and domain-adaptive continued pre-training to verify the effectiveness in different pre-training scenarios.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【27†L65-L70】",
                            "start_idx": 4640,
                            "end_idx": 4652,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2203.02155] Training language models to follow instructions with human feedback",
                            "url": "https://arxiv.org/abs/2203.02155#:~:text=reinforcement%20learning%20from%20human%20feedback,Even%20though",
                            "pub_date": null,
                            "snippet": "reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【32†L69-L77】",
                            "start_idx": 4652,
                            "end_idx": 4664,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback",
                            "url": "https://arxiv.org/abs/2212.08073#:~:text=supervised%20phase%20we%20sample%20from,judged",
                            "pub_date": null,
                            "snippet": "supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【37†L57-L62】",
                            "start_idx": 4664,
                            "end_idx": 4676,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Robust LLM safeguarding via refusal feature adversarial training",
                            "url": "https://arxiv.org/html/2409.20089v2#:~:text=model%20safety,to%20existing%20adversarial%20training%20methods",
                            "pub_date": null,
                            "snippet": "model safety. Based on these findings, we propose Re fusal F eature A dversarial T raining (ReFAT), a novel algorithm that efficiently performs LLM adversarial training by simulating the effect of input-level attacks via RFA. Experiment results show that ReFAT significantly improves the robustness of three popular LLMs against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【39†L40-L49】",
                            "start_idx": 4676,
                            "end_idx": 4688,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                            "url": "https://arxiv.org/html/2412.16504v1#:~:text=Fine,learning%2C%20and%20knowledge%20unlearning%2C%20discussing",
                            "pub_date": null,
                            "snippet": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【41†L62-L64】",
                            "start_idx": 4688,
                            "end_idx": 4700,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Fine-Tuning LLMs for Multi-Turn Conversations: A Technical Deep Dive",
                            "url": "https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=%2A%20Multi,training",
                            "pub_date": null,
                            "snippet": "* Multi-Turn Complexity: While base models handle single exchanges well, maintaining context and coherence across nuanced multistep conversations requires further specialized post-training.",
                            "attribution": "together.ai",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【52†L88-L97】",
                            "start_idx": 4700,
                            "end_idx": 4712,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "LLM Agents for Education: Advances and Applications",
                            "url": "https://arxiv.org/html/2503.11733v1#:~:text=The%20strong%20natural%20language%20understanding,retrieval%2C%20and%20adaptive%20content%20generation",
                            "pub_date": null,
                            "snippet": "The strong natural language understanding of Large Language Models (LLMs) and the task automation capabilities of LLM agents make them valuable for addressing challenges in education (Weng, 2023 ; Wang et al.,  47). First, memory enables LLM agents to retain both long-term knowledge about students’ study habits and short-term context from real-time interactions, enhancing contextual understanding and ensuring personalized learning experiences across various educational tasks (Zhang et al., 2024a). Second, tool use allows LLM agents to access external resources, perform complex calculations, and retrieve real-time information, enabling them to automate intricate educational tasks such as grading, knowledge retrieval, and adaptive content generation,",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【54†L105-L113】",
                            "start_idx": 4712,
                            "end_idx": 4726,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Current applications and challenges in large language models for patient care: a systematic review | Communications Medicine",
                            "url": "https://www.nature.com/articles/s43856-024-00717-2#:~:text=We%20show%20that%20most%20studies,comprehensiveness",
                            "pub_date": null,
                            "snippet": "We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness,",
                            "attribution": "nature.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【50†L85-L94】",
                            "start_idx": 4726,
                            "end_idx": 4738,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "A Survey on Large Language Models for Code Generation",
                            "url": "https://arxiv.org/html/2406.00515v1#:~:text=The%20advent%20of%20Large%20Language,Ren",
                            "pub_date": null,
                            "snippet": "The advent of Large Language Models (LLMs) such as ChatGPT^{1}^{1}1https://chat.openai.com (OpenAI, 2022 ) has profoundly transformed the landscape of automated code-related tasks (Chen et al.,  36), including code completion (Wang and Li, 2021 ; Lu et al.,  38; Guo et al., 2023 ; Wu et al.,  40), code translation (Lachaux et al., 2020 ; Szafraniec et al.,  42; Chen et al., 2018 ), and code repair (Jiang et al.,  44; Olausson et al., 2023 ; Parasaram et al.,  46). A particularly intriguing application of LLMs is code generation, a task that involves producing source code from natural language descriptions. Despite varying definitions across studies (Ren",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【50†L99-L102】",
                            "start_idx": 4738,
                            "end_idx": 4751,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "A Survey on Large Language Models for Code Generation",
                            "url": "https://arxiv.org/html/2406.00515v1#:~:text=2023%29,US",
                            "pub_date": null,
                            "snippet": "2023). This area has garnered substantial interest from both academia and industry, as evidenced by the development of tools like GitHub Copilot^{2}^{2}2https://github.com/features/copilot (Chen et al., 2021), CodeGeeX^{3}^{3}3https://codegeex.cn/en-US",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【43†L52-L60】",
                            "start_idx": 4751,
                            "end_idx": 4763,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                            "url": "https://arxiv.org/abs/2201.11903#:~:text=,parameter",
                            "pub_date": null,
                            "snippet": "> Abstract:We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【48†L52-L60】",
                            "start_idx": 4763,
                            "end_idx": 4775,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2312.10997] Retrieval-Augmented Generation for Large Language Models: A Survey",
                            "url": "https://arxiv.org/abs/2312.10997#:~:text=,comprehensive%20review%20paper%20offers%20a",
                            "pub_date": null,
                            "snippet": "> Abstract:Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non- transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【45†L52-L61】",
                            "start_idx": 4775,
                            "end_idx": 4787,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools",
                            "url": "https://arxiv.org/abs/2302.04761#:~:text=,A",
                            "pub_date": null,
                            "snippet": "> Abstract:Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【60†L1-L4】",
                            "start_idx": 4787,
                            "end_idx": 4797,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "GPT-4 | OpenAI",
                            "url": "https://openai.com/index/gpt-4-research/#:~:text=GPT%E2%80%914%20can%20accept%20a%20prompt,range%20of%20domains%E2%80%94including%20documents%20with",
                            "pub_date": null,
                            "snippet": "GPT‑4 can accept a prompt of text and images, which—parallel to the text-only setting—lets the user specify any vision or language task. Specifically, it generates text outputs (natural language, code, etc.) given inputs consisting of interspersed text and images. Over a range of domains—including documents with",
                            "attribution": "openai.com",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【65†L62-L71】",
                            "start_idx": 4797,
                            "end_idx": 4809,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2211.09110] Holistic Evaluation of Language Models",
                            "url": "https://arxiv.org/abs/2211.09110#:~:text=,of%2016%20core%20scenarios%20when",
                            "pub_date": null,
                            "snippet": "> Abstract:Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【63†L58-L61】",
                            "start_idx": 4809,
                            "end_idx": 4821,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2305.12474] Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
                            "url": "https://arxiv.org/abs/2305.12474#:~:text=the%20performance%20of%20LLMs,exhibit%20significant%20performance%20disparities%20across",
                            "pub_date": null,
                            "snippet": "the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and this http URL findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【73†L189-L197】",
                            "start_idx": 4821,
                            "end_idx": 4835,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "[2206.04615] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                            "url": "https://arxiv.org/abs/2206.04615#:~:text=benchmark%20%28BIG,tasks%20in%20order%20to%20provide",
                            "pub_date": null,
                            "snippet": "benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide",
                            "attribution": "arxiv.org",
                            "icon_type": null
                        },
                        {
                            "matched_text": "【75†L69-L77】",
                            "start_idx": 4835,
                            "end_idx": 4847,
                            "alt": null,
                            "type": "webpage_extended",
                            "title": "Detecting hallucinations in large language models using semantic entropy | Nature",
                            "url": "https://www.nature.com/articles/s41586-024-07421-0#:~:text=%E2%80%98hallucinate%E2%80%99%20false%20outputs%20and%20unsubstantiated,based%20uncertainty%20estimators",
                            "pub_date": null,
                            "snippet": "‘hallucinate’ false outputs and unsubstantiated answers3 , 24. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5  or untrue facts in news articles 26 and even posing a risk to human life in medical domains such as radiology7 . Encouraging truthfulness through supervision or reinforcement has been only partially successful 28. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators",
                            "attribution": "nature.com",
                            "icon_type": null
                        }
                    ],
                    "timestamp_": "absolute",
                    "message_type": null,
                    "is_async_task_result_message": true,
                    "b1de6e2_rm": true,
                    "async_task_id": "deepresch_681b9b6e4a6c8191a80f3cbf2aac7a8e",
                    "async_task_title": "大语言模型的近期研究进展综述：预训练、微调、使用与评估",
                    "model_switcher_deny": [
                        {
                            "slug": "o1-mini",
                            "context": "regenerate",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        },
                        {
                            "slug": "o1-mini",
                            "context": "conversation",
                            "reason": "unsupported_tool_use",
                            "is_available": false,
                            "description": "This model doesn't support using tools."
                        }
                    ]
                },
                "recipient": "all",
                "channel": "final"
            },
            "parent": "8b4f79e9-b11d-4d47-a362-0afae2c29e20",
            "children": []
        }
    },
    "moderation_results": [],
    "current_node": "d67f3621-b13c-4d72-ab10-f49532dbbe51",
    "plugin_ids": null,
    "conversation_id": "681b9b44-105c-8004-a588-3d4eeffd8d04",
    "conversation_template_id": null,
    "gizmo_id": null,
    "gizmo_type": null,
    "is_archived": false,
    "is_starred": null,
    "safe_urls": [],
    "blocked_urls": [],
    "default_model_slug": "research",
    "conversation_origin": null,
    "voice": null,
    "async_status": null,
    "disabled_tool_ids": [],
    "is_do_not_remember": null,
    "memory_scope": "global_enabled"
}